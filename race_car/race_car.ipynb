{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Base Implementation\n",
    "This will be the baseline implementation for comparing with the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.99\n",
    "EPOCHS = 20\n",
    "CLIP_EPSILON = 0.2\n",
    "BATCH_SIZE = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import wandb\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import LeakyReLU, Linear, MSELoss, Sequential, Softmax, Conv1d, Flatten, MaxPool1d\n",
    "from torch.optim import Adam\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WandSession:\n",
    "  def __init__(self, enabled=True):\n",
    "    self.enabled = enabled\n",
    "    if enabled:\n",
    "      wandb.init(\n",
    "          project=\"car-racing-base\",\n",
    "\n",
    "          config={\n",
    "              \"learning_rate\": LEARNING_RATE,\n",
    "              \"gamma\": GAMMA,\n",
    "              \"epochs\": EPOCHS,\n",
    "              \"clip_epsilon\": CLIP_EPSILON,\n",
    "              \"batch_size\": BATCH_SIZE,\n",
    "              \"seed\": SEED\n",
    "          },\n",
    "      )\n",
    "      \n",
    "  def log(self, *args, **kwargs):\n",
    "    if self.enabled:\n",
    "      wandb.log(args, kwargs)\n",
    "      \n",
    "  def finish(self):\n",
    "    if self.enabled:\n",
    "      wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"mps\")\n",
    "\n",
    "def create_env(**kwargs):\n",
    "  return gym.make('CarRacing-v2', continuous=False, **kwargs)\n",
    "\n",
    "def reset_env(env: gym.Env):\n",
    "  state, _ = env.reset()\n",
    "  state = state.transpose(2, 0, 1)\n",
    "  state = 0.299 * state[0] + 0.587 * state[1] + 0.114 * state[2]\n",
    "    \n",
    "  return state\n",
    "\n",
    "def step_env(env: gym.Env, action: np.ndarray):\n",
    "  state, reward, done, *_ = env.step(action)\n",
    "  state = state.transpose(2, 0, 1)\n",
    "  state = 0.299 * state[0] + 0.587 * state[1] + 0.114 * state[2]\n",
    "  \n",
    "  return state, reward, done\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "wandb_session = WandSession(enabled=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PolicyNetwork**:\n",
    "- Input: State\n",
    "- Output: Action distribution (0-1)\n",
    "- 2 Hidden layers with LeakyReLU activation\n",
    "\n",
    "**ValueNetwork**:\n",
    "- Input: State\n",
    "- Output: Value\n",
    "- 2 Hidden layers with LeakyReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(torch.nn.Module):\n",
    "  def __init__(self, hidden_dim):\n",
    "    super().__init__()\n",
    "    self.model = Sequential(\n",
    "      Conv1d(96, 16, 4, stride=4),\n",
    "      LeakyReLU(),\n",
    "      Conv1d(16, 32, 4, stride=2), #\n",
    "      LeakyReLU(),\n",
    "      MaxPool1d(2),\n",
    "      Flatten(),\n",
    "      Linear(160, hidden_dim),\n",
    "      LeakyReLU(),\n",
    "      Linear(hidden_dim, 5),\n",
    "      Softmax(dim=-1)\n",
    "    )\n",
    "\n",
    "  def forward(self, state):\n",
    "    if len(state.shape) == 2:\n",
    "      state = state.unsqueeze(0)\n",
    "    \n",
    "    return self.model(state)\n",
    "  \n",
    "  @torch.no_grad()\n",
    "  def act(self, state: np.array):\n",
    "    state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "    probs = self.model(state)\n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action.item(), m.log_prob(action)\n",
    "  \n",
    "class ValueNetwork(torch.nn.Module):\n",
    "  def __init__(self, hidden_dim) -> None:\n",
    "    super().__init__()\n",
    "    self.model = Sequential(\n",
    "      Conv1d(96, 16, 4, stride=4),\n",
    "      LeakyReLU(),\n",
    "      Conv1d(16, 32, 4, stride=2),\n",
    "      LeakyReLU(),\n",
    "      MaxPool1d(2),\n",
    "      Flatten(),\n",
    "      Linear(160, hidden_dim),\n",
    "      LeakyReLU(),\n",
    "      Linear(hidden_dim, 1),\n",
    "    )\n",
    "  \n",
    "  def forward(self, state):\n",
    "    if len(state.shape) == 2:\n",
    "      state = state.unsqueeze(0)\n",
    "    \n",
    "    return self.model(state)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "- 64 hidden nodes\n",
    "- Adam optimizer\n",
    "- MSE loss for value network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = PolicyNetwork(64).to(device)\n",
    "value_net  = ValueNetwork(64).to(device)\n",
    "\n",
    "policy_optimizer = Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "value_optimizer  = Adam(value_net.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "criterion = MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards):\n",
    "  returns = torch.zeros(len(rewards))\n",
    "  R = 0\n",
    "  for i in reversed(range(len(rewards))):\n",
    "    R = rewards[i] + GAMMA * R\n",
    "    returns[i] = R\n",
    "  return returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_step(env: gym.Env):\n",
    "    state = reset_env(env)\n",
    "    \n",
    "    # capture entire episode\n",
    "    done, steps = False, 0\n",
    "    states, actions, log_probs_old, rewards = [], [], [], []\n",
    "    \n",
    "    print(f\"Running episode: \", end=\"\", flush=True)\n",
    "    \n",
    "    while not done and sum(rewards[:7]) >= 0:\n",
    "        action, log_prob = policy_net.act(state)\n",
    "        next_state, reward, done = step_env(env, action)\n",
    "\n",
    "        log_probs_old.append(log_prob)\n",
    "        states.append(state)\n",
    "        actions.append(action)\n",
    "        rewards.append(reward)\n",
    "\n",
    "        state = next_state\n",
    "        steps += 1\n",
    "    \n",
    "    env.reset()\n",
    "    \n",
    "    print(f\"{steps} steps\")\n",
    "    # Convert to tensors\n",
    "    # Be sure to detach() the tensors from the graph as these are \"constants\"\n",
    "    states = torch.from_numpy(np.array(states).astype(\"float32\")).detach().to(device)\n",
    "    actions = torch.tensor(actions).detach().to(device)\n",
    "    log_probs_old = torch.stack(log_probs_old).detach().to(device)\n",
    "    \n",
    "    returns = compute_returns(rewards).detach().to(device)\n",
    "    \n",
    "    values = value_net(states)\n",
    "    advantages = (returns - values.squeeze()).detach().to(device)\n",
    "\n",
    "    print(f\" -- Mean reward: {np.mean(rewards)}\")\n",
    "    for _ in range(EPOCHS):\n",
    "        for i in range(0, len(states), BATCH_SIZE):\n",
    "            # Grab a batch of data\n",
    "            batch_states = states[i:i+BATCH_SIZE]\n",
    "            batch_actions = actions[i:i+BATCH_SIZE]\n",
    "            batch_log_probs_old = log_probs_old[i:i+BATCH_SIZE]\n",
    "            batch_advantages = advantages[i:i+BATCH_SIZE]\n",
    "            batch_returns = returns[i:i+BATCH_SIZE]\n",
    "\n",
    "            # Calculate new log probabilities\n",
    "            new_action_probs = policy_net(batch_states)\n",
    "            new_log_probs = torch.log(new_action_probs.gather(1, batch_actions.unsqueeze(-1)))\n",
    "\n",
    "            # rho is the ratio between new and old log probabilities\n",
    "            ratio = (new_log_probs - batch_log_probs_old).exp()\n",
    "\n",
    "            # Calculate surrogate loss\n",
    "            surrogate_loss = ratio * batch_advantages\n",
    "            clipped_surrogate_loss = torch.clamp(ratio, 1-CLIP_EPSILON, 1+CLIP_EPSILON) * batch_advantages\n",
    "            policy_loss = -torch.min(surrogate_loss, clipped_surrogate_loss).mean()\n",
    "\n",
    "            policy_optimizer.zero_grad()\n",
    "            policy_loss.backward()\n",
    "            policy_optimizer.step()\n",
    "\n",
    "            value_loss = criterion(value_net(batch_states),\n",
    "                                   batch_returns.unsqueeze(-1))\n",
    "\n",
    "            value_optimizer.zero_grad()\n",
    "            value_loss.backward()\n",
    "            value_optimizer.step()\n",
    "            \n",
    "            wandb_session.log({\n",
    "                \"policy_loss\": policy_loss.item(),\n",
    "                \"value_loss\": value_loss.item(),\n",
    "                \"steps\": steps,\n",
    "            })\n",
    "            \n",
    "    return (returns.mean(), returns.std(), steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/race_car/video/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/race_car/video/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/race_car/video/rl-video-episode-0.mp4\n",
      "Running episode: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/race_car/video/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/race_car/video/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/race_car/video/rl-video-episode-0.mp4\n",
      "411 steps\n",
      " -- Mean reward: -6.089872360229492\n",
      "Episode 0\tSteps: 411\tReturn: 411\n",
      "Running episode: 403 steps\n",
      " -- Mean reward: -2.4592158794403076\n",
      "Running episode: 1028 steps\n",
      " -- Mean reward: -14.090010643005371\n",
      "Running episode: 1380 steps\n",
      " -- Mean reward: -12.673460960388184\n",
      "Running episode: 870 steps\n",
      " -- Mean reward: -16.07280731201172\n",
      "Running episode: Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/race_car/video/rl-video-episode-5.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/race_car/video/rl-video-episode-5.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/race_car/video/rl-video-episode-5.mp4\n",
      "2664 steps\n",
      " -- Mean reward: -11.871350288391113\n",
      "Episode 5\tSteps: 2664\tReturn: 2664\n",
      "Running episode: 1873 steps\n",
      " -- Mean reward: -12.088123321533203\n",
      "Running episode: 976 steps\n",
      " -- Mean reward: -15.386592864990234\n",
      "Running episode: 639 steps\n",
      " -- Mean reward: -8.886055946350098\n",
      "Running episode: 1099 steps\n",
      " -- Mean reward: -7.066318035125732\n",
      "Running episode: Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/race_car/video/rl-video-episode-10.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/race_car/video/rl-video-episode-10.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/race_car/video/rl-video-episode-10.mp4\n",
      "931 steps\n",
      " -- Mean reward: -15.692913055419922\n",
      "Episode 10\tSteps: 931\tReturn: 931\n",
      "Running episode: "
     ]
    }
   ],
   "source": [
    "gym_env = create_env(render_mode=\"rgb_array\")\n",
    "gym_env = gym.wrappers.RecordVideo(gym_env, \"video\", episode_trigger=lambda x: x % 5 == 0)\n",
    "\n",
    "gym_env.reset()\n",
    "gym_env.start_video_recorder()\n",
    "\n",
    "for i in range(300):\n",
    "  _, _, steps = ppo_step(gym_env)\n",
    "  if i % 5 == 0:\n",
    "    print(f\"Episode {i}\\tSteps: {steps}\\tReturn: {steps}\")\n",
    "  \n",
    "gym_env.close()\n",
    "wandb_session.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def record_best_effort():\n",
    "  env = create_env()\n",
    "  env = gym.wrappers.RecordVideo(env, \"tests\")\n",
    "\n",
    "  state, _ = env.reset()\n",
    "  env.start_video_recorder()\n",
    "\n",
    "  total_reward = 0\n",
    "  done, i = False, 0\n",
    "  \n",
    "  while not done and i < 10000:\n",
    "    action, _ = policy_net.deterministic_action(state)\n",
    "    state, reward, done, *_ = env.step(action)\n",
    "    total_reward += reward\n",
    "    i += 1\n",
    "\n",
    "  env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "record_best_effort()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
