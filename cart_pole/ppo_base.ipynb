{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Base Implementation\n",
    "This will be the baseline implementation for comparing with the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import LeakyReLU, ReLU, Linear, MSELoss, Sequential, Softmax, Dropout\n",
    "from torch.optim import Adam\n",
    "from torch.quantization import quantize_dynamic\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.99\n",
    "EPOCHS = 1\n",
    "CLIP_EPSILON = 0.2\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "TYPE = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(TYPE)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "# torch.backends.cpu.deterministic = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, extras: dict):\n",
    "        self.data = []\n",
    "        self.base_time = time.time()\n",
    "        self.extras = extras\n",
    "        \n",
    "    def add(self, p_loss, v_loss, steps):\n",
    "        if len(self.data) == 0:\n",
    "            self.base_time = time.time()\n",
    "        \n",
    "        time_elapsed = round(time.time() - self.base_time, 5)\n",
    "        self.data.append((time_elapsed, p_loss, v_loss, steps))\n",
    "    \n",
    "    def df(self):\n",
    "        return pd.DataFrame(self.data, columns=[\"time\", \"p_loss\", \"v_loss\", \"steps\"])\n",
    "    \n",
    "    def save(self):\n",
    "        path = \"logs/\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        os.mkdir(path)\n",
    "        pd.DataFrame([self.extras]).to_csv(path + \"/info.csv\")\n",
    "        pd.DataFrame(\n",
    "            self.data, \n",
    "            columns=[\"time\", \"p_loss\", \"v_loss\", \"steps\"]\n",
    "        ).to_csv(path + \"/data.csv\")\n",
    "\n",
    "logger = Logger({\n",
    "    \"seed\": SEED,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"gamma\": GAMMA,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"clip_epsilon\": CLIP_EPSILON,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"device\": DEVICE,\n",
    "    \"type\": TYPE\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleQuantized:\n",
    "  def __init__(self, dtype: torch.dtype, device: torch.device):\n",
    "    self.dtype = dtype\n",
    "    self.device = device\n",
    "    \n",
    "    env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "    env = gym.wrappers.RecordVideo(env, f\"training/{dtype}/\", episode_trigger=lambda x: x % 100 == 0 and x >= 30)\n",
    "    env.reset()\n",
    "    env.start_video_recorder()\n",
    "    \n",
    "    self.env = env\n",
    "  \n",
    "  def reset(self):\n",
    "    return torch.tensor(self.env.reset()[0], dtype=self.dtype, device=self.device)\n",
    "  \n",
    "  def step(self, action):\n",
    "    state, reward, done, truncated, _ = self.env.step(action)\n",
    "    return torch.tensor(state, dtype=self.dtype, device=self.device), reward, done or truncated\n",
    "  \n",
    "  def close(self):\n",
    "    self.env.close()\n",
    "  \n",
    "  @staticmethod\n",
    "  def env():\n",
    "    return gym.make('CartPole-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PolicyNetwork**:\n",
    "- Input: State\n",
    "- Output: Action distribution (0-1)\n",
    "- 2 Hidden layers with LeakyReLU activation\n",
    "\n",
    "**ValueNetwork**:\n",
    "- Input: State\n",
    "- Output: Value\n",
    "- 2 Hidden layers with LeakyReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolicyNetwork(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim):\n",
    "    super().__init__()\n",
    "    self.model = Sequential(\n",
    "      Linear(input_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, 2),\n",
    "      Softmax()\n",
    "    )\n",
    "\n",
    "  def forward(self, state): \n",
    "    return self.model(state)\n",
    " \n",
    "  def stochastic_action(self, state):\n",
    "    r\"\"\"Returns an action sampled from the policy network.\"\"\"\n",
    "    \n",
    "    probs = self.forward(state).detach()\n",
    "    # adding floating point error to the maximum probability\n",
    "    probs[torch.argmax(probs)] += 1 - probs.sum()\n",
    "    \n",
    "    probs.squeeze() # quantized tensors have an extra dimension\n",
    "    \n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action.item(), m.log_prob(action)\n",
    "  \n",
    "  def deterministic_action(self, state):\n",
    "    r\"\"\"Returns an action with the highest probability.\"\"\"\n",
    "    \n",
    "    probs = self.forward(state).detach()\n",
    "    action = torch.argmax(probs)\n",
    "    return action.item()\n",
    "\n",
    "  \n",
    "class ValueNetwork(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim) -> None:\n",
    "    super().__init__()\n",
    "    self.model = Sequential(\n",
    "      Linear(input_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, 1)\n",
    "    )\n",
    "  \n",
    "  def forward(self, state):\n",
    "    return self.model(state)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "- 64 hidden nodes\n",
    "- Adam optimizer\n",
    "- MSE loss for value network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOSession:\n",
    "    def __init__(self, env: CartPoleQuantized):\n",
    "        self.env = env\n",
    "        self.episode = 0\n",
    "        \n",
    "        _observation_size = CartPoleQuantized.env().observation_space.shape[0]\n",
    "\n",
    "        self.policy_net = PolicyNetwork(_observation_size, 64).to(DEVICE)\n",
    "        self.value_net  = ValueNetwork(_observation_size, 64).to(DEVICE)\n",
    "\n",
    "        self.policy_optimizer = Adam(self.policy_net.parameters(), lr=LEARNING_RATE, eps=1e-7)\n",
    "        self.value_optimizer  = Adam(self.value_net.parameters(), lr=LEARNING_RATE, eps=1e-7)\n",
    "        self.quantized = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_returns(rewards):\n",
    "        returns = torch.zeros(len(rewards))\n",
    "        R = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = rewards[i] + GAMMA * R\n",
    "            returns[i] = R\n",
    "        return returns\n",
    "    \n",
    "    def run(self, episodes):\n",
    "        self.policy_net.train()\n",
    "        self.value_net.train()\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            self.episode += 1\n",
    "            returns, std, steps = self.ppo_step()\n",
    "            \n",
    "            if self.episode % 5 == 0:\n",
    "                print(f\"Episode {self.episode} - Returns: {returns} - Std: {std} - Steps: {steps}\")\n",
    "\n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "    \n",
    "    def ppo_step(self):\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        # capture entire episode\n",
    "        done, steps = False, 0\n",
    "        states, actions, log_probs_old, rewards = [], [], [], []\n",
    "        \n",
    "        while not done:\n",
    "            if self.quantized:\n",
    "                state = state.unsqueeze(0)\n",
    "            action, log_prob = self.policy_net.stochastic_action(state)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "\n",
    "            log_probs_old.append(log_prob)\n",
    "            states.append(state.squeeze())\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        \n",
    "        # Convert to tensors\n",
    "        # Be sure to detach() the tensors from the graph as these are \"constants\"\n",
    "        states = torch.stack(states).detach().to(DEVICE)\n",
    "        actions = torch.tensor(actions).detach().to(DEVICE)\n",
    "        log_probs_old = torch.stack(log_probs_old).detach().to(DEVICE)\n",
    "        \n",
    "        returns = self.compute_returns(rewards).detach().to(DEVICE)\n",
    "        values = self.value_net(states).detach().to(DEVICE)\n",
    "        advantages = returns - values.squeeze()\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            for i in range(0, len(states), BATCH_SIZE):\n",
    "                # Grab a batch of data\n",
    "                batch_states = states[i:i+BATCH_SIZE]\n",
    "                batch_actions = actions[i:i+BATCH_SIZE]\n",
    "                batch_log_probs_old = log_probs_old[i:i+BATCH_SIZE]\n",
    "                batch_advantages = advantages[i:i+BATCH_SIZE]\n",
    "                batch_returns = returns[i:i+BATCH_SIZE]\n",
    "\n",
    "                # Calculate new log probabilities\n",
    "                new_action_probs = self.policy_net(batch_states)\n",
    "                new_log_probs = torch.log(new_action_probs.gather(1, batch_actions.unsqueeze(-1)))\n",
    "\n",
    "                # rho is the ratio between new and old log probabilities\n",
    "                ratio = (new_log_probs - batch_log_probs_old).exp()\n",
    "\n",
    "                # Calculate surrogate loss\n",
    "                surrogate_loss = ratio * batch_advantages\n",
    "                clipped_surrogate_loss = torch.clamp(ratio, 1-CLIP_EPSILON, 1+CLIP_EPSILON) * batch_advantages\n",
    "                policy_loss = -torch.min(surrogate_loss, clipped_surrogate_loss).mean()\n",
    "                \n",
    "                self.policy_optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                self.policy_optimizer.step()\n",
    "\n",
    "                # check for nan\n",
    "                if torch.isnan(policy_loss):\n",
    "                    print(\"NaN detected in policy loss\")\n",
    "                    return\n",
    "\n",
    "                value_loss = torch.pow(self.value_net(\n",
    "                    batch_states) - batch_returns.unsqueeze(-1), 2).mean()\n",
    "\n",
    "                self.value_optimizer.zero_grad()\n",
    "                value_loss.backward()\n",
    "                self.value_optimizer.step()\n",
    "\n",
    "                # check for nan\n",
    "                if torch.isnan(value_loss):\n",
    "                    print(\"NaN detected in value loss\")\n",
    "                    return\n",
    "        \n",
    "        logger.add(policy_loss.item(), value_loss.item(), steps)\n",
    "        return (returns.mean(), returns.std(), steps)\n",
    "    \n",
    "    def record_best_effort(self):\n",
    "        env = gym.make('CartPole-v1', render_mode='rgb_array', max_episode_steps=10000)\n",
    "        env = gym.wrappers.RecordVideo(env, \"tests\")\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=TYPE, device=DEVICE, requires_grad=False)\n",
    "        env.start_video_recorder()\n",
    "\n",
    "        total_reward = 0\n",
    "        done, i = False, 0\n",
    "        \n",
    "        while not done and not truncated:\n",
    "            if self.quantized:\n",
    "                state = state.unsqueeze(0)\n",
    "            \n",
    "            action = self.policy_net.deterministic_action(state)\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            state = torch.tensor(state, dtype=TYPE, device=DEVICE, requires_grad=False).unsqueeze(0)\n",
    "            total_reward += reward\n",
    "            i += 1\n",
    "\n",
    "        env.close()\n",
    "        return total_reward, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/.local/lib/python3.8/site-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = CartPoleQuantized(TYPE, DEVICE)\n",
    "session = PPOSession(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n",
      "Episode 5 - Returns: 9.863934516906738 - Std: 5.357532978057861 - Steps: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 10 - Returns: 17.297117233276367 - Std: 9.225837707519531 - Steps: 38\n",
      "Episode 15 - Returns: 7.638513565063477 - Std: 4.150516033172607 - Steps: 15\n",
      "Episode 20 - Returns: 7.638513565063477 - Std: 4.150516033172607 - Steps: 15\n",
      "Episode 25 - Returns: 9.863934516906738 - Std: 5.357532978057861 - Steps: 20\n",
      "Episode 30 - Returns: 5.804428577423096 - Std: 3.1395034790039062 - Steps: 11\n",
      "Episode 35 - Returns: 17.297117233276367 - Std: 9.225837707519531 - Steps: 38\n",
      "Episode 40 - Returns: 11.592232704162598 - Std: 6.279765605926514 - Steps: 24\n",
      "Episode 45 - Returns: 14.509786605834961 - Std: 7.805652618408203 - Steps: 31\n",
      "Episode 50 - Returns: 6.727548599243164 - Std: 3.650186777114868 - Steps: 13\n",
      "Episode 55 - Returns: 13.275749206542969 - Std: 7.165064334869385 - Steps: 28\n",
      "Episode 60 - Returns: 10.7337646484375 - Std: 5.823357105255127 - Steps: 22\n",
      "Episode 65 - Returns: 24.941802978515625 - Std: 12.918790817260742 - Steps: 59\n",
      "Episode 70 - Returns: 6.727548599243164 - Std: 3.650186777114868 - Steps: 13\n",
      "Episode 75 - Returns: 12.858996391296387 - Std: 6.9471235275268555 - Steps: 27\n",
      "Episode 80 - Returns: 8.982568740844727 - Std: 4.88210391998291 - Steps: 18\n",
      "Episode 85 - Returns: 16.513635635375977 - Std: 8.830445289611816 - Steps: 36\n",
      "Episode 90 - Returns: 20.331605911254883 - Std: 10.728294372558594 - Steps: 46\n",
      "Episode 95 - Returns: 11.164408683776855 - Std: 6.052726745605469 - Steps: 23\n",
      "Episode 100 - Returns: 11.592232704162598 - Std: 6.279765605926514 - Steps: 24\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-100.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-100.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-100.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 105 - Returns: 18.83395767211914 - Std: 9.992586135864258 - Steps: 42\n",
      "Episode 110 - Returns: 25.61774253845215 - Std: 13.230171203613281 - Steps: 61\n",
      "Episode 115 - Returns: 21.066091537475586 - Std: 11.084874153137207 - Steps: 48\n",
      "Episode 120 - Returns: 12.858996391296387 - Std: 6.9471235275268555 - Steps: 27\n",
      "Episode 125 - Returns: 7.184538841247559 - Std: 3.901630163192749 - Steps: 14\n",
      "Episode 130 - Returns: 8.982568740844727 - Std: 4.88210391998291 - Steps: 18\n",
      "Episode 135 - Returns: 25.280845642089844 - Std: 13.075297355651855 - Steps: 60\n",
      "Episode 140 - Returns: 29.49863624572754 - Std: 14.966854095458984 - Steps: 73\n",
      "Episode 145 - Returns: 24.60059928894043 - Std: 12.760636329650879 - Steps: 58\n",
      "Episode 150 - Returns: 19.211999893188477 - Std: 10.179376602172852 - Steps: 43\n",
      "Episode 155 - Returns: 29.18631362915039 - Std: 14.83039665222168 - Steps: 72\n",
      "Episode 160 - Returns: 21.4298095703125 - Std: 11.260408401489258 - Steps: 49\n",
      "Episode 165 - Returns: 26.28515625 - Std: 13.53508472442627 - Steps: 63\n",
      "Episode 170 - Returns: 36.97296142578125 - Std: 18.046926498413086 - Steps: 99\n",
      "Episode 175 - Returns: 40.28580856323242 - Std: 19.28926658630371 - Steps: 112\n",
      "Episode 180 - Returns: 31.031234741210938 - Std: 15.627799987792969 - Steps: 78\n",
      "Episode 185 - Returns: 31.33205223083496 - Std: 15.755814552307129 - Steps: 79\n",
      "Episode 190 - Returns: 47.21794891357422 - Std: 21.61115264892578 - Steps: 143\n",
      "Episode 195 - Returns: 23.911640167236328 - Std: 12.439318656921387 - Steps: 56\n",
      "Episode 200 - Returns: 37.49982833862305 - Std: 18.249834060668945 - Steps: 101\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-200.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-200.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-200.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 205 - Returns: 41.492225646972656 - Std: 19.721296310424805 - Steps: 117\n",
      "Episode 210 - Returns: 39.04246520996094 - Std: 18.83245277404785 - Steps: 107\n",
      "Episode 215 - Returns: 51.60670471191406 - Std: 22.862524032592773 - Steps: 166\n",
      "Episode 220 - Returns: 48.023681640625 - Std: 21.85443878173828 - Steps: 147\n",
      "Episode 225 - Returns: 41.729190826416016 - Std: 19.80483055114746 - Steps: 118\n",
      "Episode 230 - Returns: 56.38336181640625 - Std: 24.00244903564453 - Steps: 195\n",
      "Episode 235 - Returns: 40.530006408691406 - Std: 19.37761688232422 - Steps: 113\n",
      "Episode 240 - Returns: 62.802345275878906 - Std: 25.107769012451172 - Steps: 243\n",
      "Episode 245 - Returns: 38.53449249267578 - Std: 18.642515182495117 - Steps: 105\n",
      "Episode 250 - Returns: 48.023681640625 - Std: 21.85443878173828 - Steps: 147\n",
      "Episode 255 - Returns: 54.822811126708984 - Std: 23.657377243041992 - Steps: 185\n",
      "Episode 260 - Returns: 55.61387252807617 - Std: 23.83574867248535 - Steps: 190\n",
      "Episode 265 - Returns: 63.15202331542969 - Std: 25.15205192565918 - Steps: 246\n",
      "Episode 270 - Returns: 69.31168365478516 - Std: 25.615415573120117 - Steps: 308\n",
      "Episode 275 - Returns: 70.30342864990234 - Std: 25.627948760986328 - Steps: 320\n",
      "Episode 280 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 285 - Returns: 56.38336181640625 - Std: 24.00244903564453 - Steps: 195\n",
      "Episode 290 - Returns: 57.13199234008789 - Std: 24.15803337097168 - Steps: 200\n",
      "Episode 295 - Returns: 62.08643341064453 - Std: 25.01162338256836 - Steps: 237\n",
      "Episode 300 - Returns: 64.49800109863281 - Std: 25.305606842041016 - Steps: 258\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-300.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-300.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-300.mp4\n",
      "Episode 305 - Returns: 53.51039505004883 - Std: 23.34624481201172 - Steps: 177\n",
      "Episode 310 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 315 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 320 - Returns: 74.33402252197266 - Std: 25.469770431518555 - Steps: 377\n",
      "Episode 325 - Returns: 74.94189453125 - Std: 25.41397476196289 - Steps: 387\n",
      "Episode 330 - Returns: 75.63785552978516 - Std: 25.338838577270508 - Steps: 399\n",
      "Episode 335 - Returns: 79.45845031738281 - Std: 24.69032859802246 - Steps: 478\n",
      "Episode 340 - Returns: 73.6341552734375 - Std: 25.52305030822754 - Steps: 366\n",
      "Episode 345 - Returns: 69.89702606201172 - Std: 25.625102996826172 - Steps: 315\n",
      "Episode 350 - Returns: 63.38215255737305 - Std: 25.180221557617188 - Steps: 248\n",
      "Episode 355 - Returns: 45.33467102050781 - Std: 21.02001953125 - Steps: 134\n",
      "Episode 360 - Returns: 77.95990753173828 - Std: 24.995126724243164 - Steps: 444\n",
      "Episode 365 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 370 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 375 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 380 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 385 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 390 - Returns: 75.23619079589844 - Std: 25.38369369506836 - Steps: 392\n",
      "Episode 395 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 400 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-400.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-400.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-400.mp4\n",
      "Episode 405 - Returns: 76.61822509765625 - Std: 25.21173858642578 - Steps: 417\n",
      "Episode 410 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 415 - Returns: 60.32577896118164 - Std: 24.74488067626953 - Steps: 223\n",
      "Episode 420 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 425 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 430 - Returns: 73.95645904541016 - Std: 25.49994659423828 - Steps: 371\n",
      "Episode 435 - Returns: 65.24596405029297 - Std: 25.37897300720215 - Steps: 265\n",
      "Episode 440 - Returns: 63.03606414794922 - Std: 25.137563705444336 - Steps: 245\n",
      "Episode 445 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 450 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 455 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 460 - Returns: 78.86508178710938 - Std: 24.81931495666504 - Steps: 464\n",
      "Episode 465 - Returns: 79.54067993164062 - Std: 24.671567916870117 - Steps: 480\n",
      "Episode 470 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 475 - Returns: 75.97284698486328 - Std: 25.298259735107422 - Steps: 405\n",
      "Episode 480 - Returns: 75.23619079589844 - Std: 25.38369369506836 - Steps: 392\n",
      "Episode 485 - Returns: 79.74357604980469 - Std: 24.62433433532715 - Steps: 485\n",
      "Episode 490 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 495 - Returns: 67.0528793334961 - Std: 25.519105911254883 - Steps: 283\n",
      "Episode 500 - Returns: 67.80571746826172 - Std: 25.561294555664062 - Steps: 291\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-500.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-500.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-500.mp4\n",
      "Episode 505 - Returns: 58.146331787109375 - Std: 24.358173370361328 - Steps: 207\n",
      "Episode 510 - Returns: 67.80571746826172 - Std: 25.561294555664062 - Steps: 291\n",
      "Episode 515 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 520 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 525 - Returns: 78.86508178710938 - Std: 24.81931495666504 - Steps: 464\n",
      "Episode 530 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 535 - Returns: 68.17127227783203 - Std: 25.57819938659668 - Steps: 295\n",
      "Episode 540 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 545 - Returns: 56.07810974121094 - Std: 23.9371337890625 - Steps: 193\n",
      "Episode 550 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 555 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 560 - Returns: 68.8814697265625 - Std: 25.60418128967285 - Steps: 303\n",
      "Episode 565 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 570 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 575 - Returns: 76.67057800292969 - Std: 25.2042293548584 - Steps: 418\n",
      "Episode 580 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 585 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 590 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 595 - Returns: 80.33009338378906 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 600 - Returns: 76.67057800292969 - Std: 25.2042293548584 - Steps: 418\n"
     ]
    }
   ],
   "source": [
    "session.run(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'truncated' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7dc58757ea2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_best_effort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-49d488084d41>\u001b[0m in \u001b[0;36mrecord_best_effort\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'truncated' referenced before assignment"
     ]
    }
   ],
   "source": [
    "session.record_best_effort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and saving the new checkpoints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "torch.save(session.policy_net.state_dict(), \"float-16-good-policy.pt\")\n",
    "torch.save(session.value_net.state_dict(), \"float-16-good-value.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/b0kch01/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m quantize_dynamic(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     session\u001b[39m.\u001b[39mpolicy_net,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     {torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mLinear},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mqint8,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m session\u001b[39m.\u001b[39mquantized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m session\u001b[39m.\u001b[39;49mrun(\u001b[39m500\u001b[39;49m)\n",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(episodes):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     returns, std, steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mppo_step()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpisode \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode\u001b[39m}\u001b[39;00m\u001b[39m - Returns: \u001b[39m\u001b[39m{\u001b[39;00mreturns\u001b[39m}\u001b[39;00m\u001b[39m - Std: \u001b[39m\u001b[39m{\u001b[39;00mstd\u001b[39m}\u001b[39;00m\u001b[39m - Steps: \u001b[39m\u001b[39m{\u001b[39;00msteps\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m policy_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mmin(surrogate_loss, clipped_surrogate_loss)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m policy_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m value_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpow(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_net(batch_states) \u001b[39m-\u001b[39m batch_returns\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Load weights\n",
    "env = CartPoleQuantized(TYPE, DEVICE)\n",
    "session = PPOSession(env)\n",
    "session.policy_net.load_state_dict(torch.load(\"checkpoints/float-32-good-policy.pt\"))\n",
    "session.value_net.load_state_dict(torch.load(\"checkpoints/float-32-good-value.pt\"))\n",
    "\n",
    "session.policy_net.eval()\n",
    "session.value_net.eval()\n",
    "\n",
    "torch.backends.quantized.engine = \"qnnpack\"\n",
    "\n",
    "quantize_dynamic(\n",
    "    session.policy_net,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "session.quantized = True\n",
    "session.run(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15321.0, 15321)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#session.record_best_effort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
