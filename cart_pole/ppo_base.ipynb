{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Base Implementation\n",
    "This will be the baseline implementation for comparing with the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import wandb\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import LeakyReLU, ReLU, Linear, MSELoss, Sequential, Softmax, Dropout\n",
    "from torch.optim import Adam\n",
    "from torch.quantization import quantize_dynamic\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.99\n",
    "EPOCHS = 1\n",
    "CLIP_EPSILON = 0.2\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "DEVICE = torch.device(\"mps\")\n",
    "TYPE = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.set_default_dtype(TYPE)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cpu.deterministic = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class WandbLogger:\n",
    "  def __init__(self, enable=False):\n",
    "    self.enable = enable\n",
    "    \n",
    "    if enable:\n",
    "      wandb.init(\n",
    "          project=\"ppo-base-float16\",\n",
    "\n",
    "          config={\n",
    "              \"learning_rate\": LEARNING_RATE,\n",
    "              \"gamma\": GAMMA,\n",
    "              \"epochs\": EPOCHS,\n",
    "              \"clip_epsilon\": CLIP_EPSILON,\n",
    "              \"batch_size\": BATCH_SIZE,\n",
    "              \"seed\": SEED\n",
    "          },\n",
    "      )\n",
    "\n",
    "  def log(self, **kwargs):\n",
    "    if self.enable:\n",
    "      wandb.log(kwargs)\n",
    "      \n",
    "  def finish(self):\n",
    "    if self.enable:\n",
    "      wandb.finish()\n",
    "      \n",
    "log = WandbLogger(enable=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleQuantized:\n",
    "  def __init__(self, dtype: torch.dtype, device: torch.device):\n",
    "    self.dtype = dtype\n",
    "    self.device = device\n",
    "    \n",
    "    env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "    env = gym.wrappers.RecordVideo(env, f\"training/{dtype}/\", episode_trigger=lambda x: x % 100 == 0 and x >= 30)\n",
    "    env.reset()\n",
    "    env.start_video_recorder()\n",
    "    \n",
    "    self.env = env\n",
    "  \n",
    "  def reset(self):\n",
    "    return torch.tensor(self.env.reset()[0], dtype=self.dtype, device=self.device)\n",
    "  \n",
    "  def step(self, action):\n",
    "    state, reward, done, truncated, _ = self.env.step(action)\n",
    "    return torch.tensor(state, dtype=self.dtype, device=self.device), reward, done or truncated\n",
    "  \n",
    "  def close(self):\n",
    "    self.env.close()\n",
    "  \n",
    "  @staticmethod\n",
    "  def env():\n",
    "    return gym.make('CartPole-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PolicyNetwork**:\n",
    "- Input: State\n",
    "- Output: Action distribution (0-1)\n",
    "- 2 Hidden layers with LeakyReLU activation\n",
    "\n",
    "**ValueNetwork**:\n",
    "- Input: State\n",
    "- Output: Value\n",
    "- 2 Hidden layers with LeakyReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolicyNetwork(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim):\n",
    "    super().__init__()\n",
    "    self.model = Sequential(\n",
    "      Linear(input_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, 2),\n",
    "      Softmax()\n",
    "    )\n",
    "\n",
    "  def forward(self, state): \n",
    "    return self.model(state)\n",
    " \n",
    "  def stochastic_action(self, state):\n",
    "    r\"\"\"Returns an action sampled from the policy network.\"\"\"\n",
    "    \n",
    "    probs = self.forward(state).detach()\n",
    "    # adding floating point error to the maximum probability\n",
    "    probs[torch.argmax(probs)] += 1 - probs.sum()\n",
    "    \n",
    "    probs.squeeze() # quantized tensors have an extra dimension\n",
    "    \n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action.item(), m.log_prob(action)\n",
    "  \n",
    "  def deterministic_action(self, state):\n",
    "    r\"\"\"Returns an action with the highest probability.\"\"\"\n",
    "    \n",
    "    probs = self.forward(state).detach()\n",
    "    action = torch.argmax(probs)\n",
    "    return action.item()\n",
    "\n",
    "  \n",
    "class ValueNetwork(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim) -> None:\n",
    "    super().__init__()\n",
    "    self.model = Sequential(\n",
    "      Linear(input_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, 1)\n",
    "    )\n",
    "  \n",
    "  def forward(self, state):\n",
    "    return self.model(state)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "- 64 hidden nodes\n",
    "- Adam optimizer\n",
    "- MSE loss for value network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOSession:\n",
    "    def __init__(self, env: CartPoleQuantized):\n",
    "        self.env = env\n",
    "        self.episode = 0\n",
    "        \n",
    "        _observation_size = CartPoleQuantized.env().observation_space.shape[0]\n",
    "\n",
    "        self.policy_net = PolicyNetwork(_observation_size, 64).to(DEVICE)\n",
    "        self.value_net  = ValueNetwork(_observation_size, 64).to(DEVICE)\n",
    "\n",
    "        self.policy_optimizer = Adam(self.policy_net.parameters(), lr=LEARNING_RATE, eps=1e-7)\n",
    "        self.value_optimizer  = Adam(self.value_net.parameters(), lr=LEARNING_RATE, eps=1e-7)\n",
    "        self.quantized = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_returns(rewards):\n",
    "        returns = torch.zeros(len(rewards))\n",
    "        R = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = rewards[i] + GAMMA * R\n",
    "            returns[i] = R\n",
    "        return returns\n",
    "    \n",
    "    def run(self, episodes):\n",
    "        self.policy_net.train()\n",
    "        self.value_net.train()\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            self.episode += 1\n",
    "            returns, std, steps = self.ppo_step()\n",
    "            \n",
    "            if self.episode % 5 == 0:\n",
    "                print(f\"Episode {self.episode} - Returns: {returns} - Std: {std} - Steps: {steps}\")\n",
    "\n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "    \n",
    "    def ppo_step(self):\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        # capture entire episode\n",
    "        done, steps = False, 0\n",
    "        states, actions, log_probs_old, rewards = [], [], [], []\n",
    "        \n",
    "        while not done:\n",
    "            if self.quantized:\n",
    "                state = state.unsqueeze(0)\n",
    "            action, log_prob = self.policy_net.stochastic_action(state)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "\n",
    "            log_probs_old.append(log_prob)\n",
    "            states.append(state.squeeze())\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        \n",
    "        # Convert to tensors\n",
    "        # Be sure to detach() the tensors from the graph as these are \"constants\"\n",
    "        states = torch.stack(states).detach().to(DEVICE)\n",
    "        actions = torch.tensor(actions).detach().to(DEVICE)\n",
    "        log_probs_old = torch.stack(log_probs_old).detach().to(DEVICE)\n",
    "        \n",
    "        returns = self.compute_returns(rewards).detach().to(DEVICE)\n",
    "        values = self.value_net(states).detach().to(DEVICE)\n",
    "        advantages = returns - values.squeeze()\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            for i in range(0, len(states), BATCH_SIZE):\n",
    "                # Grab a batch of data\n",
    "                batch_states = states[i:i+BATCH_SIZE]\n",
    "                batch_actions = actions[i:i+BATCH_SIZE]\n",
    "                batch_log_probs_old = log_probs_old[i:i+BATCH_SIZE]\n",
    "                batch_advantages = advantages[i:i+BATCH_SIZE]\n",
    "                batch_returns = returns[i:i+BATCH_SIZE]\n",
    "\n",
    "                # Calculate new log probabilities\n",
    "                new_action_probs = self.policy_net(batch_states)\n",
    "                new_log_probs = torch.log(new_action_probs.gather(1, batch_actions.unsqueeze(-1)))\n",
    "\n",
    "                # rho is the ratio between new and old log probabilities\n",
    "                ratio = (new_log_probs - batch_log_probs_old).exp()\n",
    "\n",
    "                # Calculate surrogate loss\n",
    "                surrogate_loss = ratio * batch_advantages\n",
    "                clipped_surrogate_loss = torch.clamp(ratio, 1-CLIP_EPSILON, 1+CLIP_EPSILON) * batch_advantages\n",
    "                policy_loss = -torch.min(surrogate_loss, clipped_surrogate_loss).mean()\n",
    "                \n",
    "                self.policy_optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                self.policy_optimizer.step()\n",
    "\n",
    "                # check for nan\n",
    "                if torch.isnan(policy_loss):\n",
    "                    print(\"NaN detected in policy loss\")\n",
    "                    return\n",
    "\n",
    "                value_loss = torch.pow(self.value_net(\n",
    "                    batch_states) - batch_returns.unsqueeze(-1), 2).mean()\n",
    "\n",
    "                self.value_optimizer.zero_grad()\n",
    "                value_loss.backward()\n",
    "                self.value_optimizer.step()\n",
    "\n",
    "                # check for nan\n",
    "                if torch.isnan(value_loss):\n",
    "                    print(\"NaN detected in value loss\")\n",
    "                    return\n",
    "                \n",
    "        return (returns.mean(), returns.std(), steps)\n",
    "    \n",
    "    def record_best_effort(self):\n",
    "        env = gym.make('CartPole-v1', render_mode='rgb_array', max_episode_steps=10000)\n",
    "        env = gym.wrappers.RecordVideo(env, \"tests\")\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=TYPE, device=DEVICE, requires_grad=False)\n",
    "        env.start_video_recorder()\n",
    "\n",
    "        total_reward = 0\n",
    "        done, i = False, 0\n",
    "        \n",
    "        while not done or not truncated:\n",
    "            if self.quantized:\n",
    "                state = state.unsqueeze(0)\n",
    "            \n",
    "            action = self.policy_net.deterministic_action(state)\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            state = torch.tensor(state, dtype=TYPE, device=DEVICE, requires_grad=False).unsqueeze(0)\n",
    "            total_reward += reward\n",
    "            i += 1\n",
    "\n",
    "        env.close()\n",
    "        return total_reward, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/b0kch01/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/gym/wrappers/record_video.py:75: UserWarning: \u001b[33mWARN: Overwriting existing videos at /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float16 folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = CartPoleQuantized(TYPE, DEVICE)\n",
    "session = PPOSession(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/b0kch01/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 10 - Returns: 5.33984375 - Std: 2.880859375 - Steps: 10\n",
      "Episode 15 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 20 - Returns: 5.33984375 - Std: 2.880859375 - Steps: 10\n",
      "Episode 25 - Returns: 5.33984375 - Std: 2.880859375 - Steps: 10\n",
      "Episode 30 - Returns: 4.3984375 - Std: 2.353515625 - Steps: 8\n",
      "Episode 35 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 40 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 45 - Returns: 5.33984375 - Std: 2.880859375 - Steps: 10\n",
      "Episode 50 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 55 - Returns: 5.33984375 - Std: 2.880859375 - Steps: 10\n",
      "Episode 60 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 65 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 70 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 75 - Returns: 5.33984375 - Std: 2.880859375 - Steps: 10\n",
      "Episode 80 - Returns: 5.33984375 - Std: 2.880859375 - Steps: 10\n",
      "Episode 85 - Returns: 4.3984375 - Std: 2.353515625 - Steps: 8\n",
      "Episode 90 - Returns: 4.3984375 - Std: 2.353515625 - Steps: 8\n",
      "Episode 95 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 100 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-100.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-100.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-100.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 105 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 110 - Returns: 5.33984375 - Std: 2.880859375 - Steps: 10\n",
      "Episode 115 - Returns: 5.33984375 - Std: 2.880859375 - Steps: 10\n",
      "Episode 120 - Returns: 5.33984375 - Std: 2.880859375 - Steps: 10\n",
      "Episode 125 - Returns: 4.3984375 - Std: 2.353515625 - Steps: 8\n",
      "Episode 130 - Returns: 4.3984375 - Std: 2.353515625 - Steps: 8\n",
      "Episode 135 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 140 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 145 - Returns: 5.33984375 - Std: 2.880859375 - Steps: 10\n",
      "Episode 150 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 155 - Returns: 4.3984375 - Std: 2.353515625 - Steps: 8\n",
      "Episode 160 - Returns: 5.33984375 - Std: 2.880859375 - Steps: 10\n",
      "Episode 165 - Returns: 5.33984375 - Std: 2.880859375 - Steps: 10\n",
      "Episode 170 - Returns: 4.3984375 - Std: 2.353515625 - Steps: 8\n",
      "Episode 175 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 180 - Returns: 5.33984375 - Std: 2.880859375 - Steps: 10\n",
      "Episode 185 - Returns: 5.33984375 - Std: 2.880859375 - Steps: 10\n",
      "Episode 190 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 195 - Returns: 5.33984375 - Std: 2.880859375 - Steps: 10\n",
      "Episode 200 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter probs (Tensor of shape (2,)) of distribution Categorical(probs: torch.Size([2])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([nan, nan], device='mps:0')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 12\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m session\u001b[39m.\u001b[39;49mrun(\u001b[39m2500\u001b[39;49m)\n",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X14sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(episodes):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X14sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X14sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     returns, std, steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mppo_step()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X14sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X14sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpisode \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode\u001b[39m}\u001b[39;00m\u001b[39m - Returns: \u001b[39m\u001b[39m{\u001b[39;00mreturns\u001b[39m}\u001b[39;00m\u001b[39m - Std: \u001b[39m\u001b[39m{\u001b[39;00mstd\u001b[39m}\u001b[39;00m\u001b[39m - Steps: \u001b[39m\u001b[39m{\u001b[39;00msteps\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X14sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquantized:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X14sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m     state \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X14sZmlsZQ%3D%3D?line=47'>48</a>\u001b[0m action, log_prob \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpolicy_net\u001b[39m.\u001b[39;49mstochastic_action(state)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X14sZmlsZQ%3D%3D?line=48'>49</a>\u001b[0m next_state, reward, done \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39menv\u001b[39m.\u001b[39mstep(action)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X14sZmlsZQ%3D%3D?line=50'>51</a>\u001b[0m log_probs_old\u001b[39m.\u001b[39mappend(log_prob)\n",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X14sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m probs[torch\u001b[39m.\u001b[39margmax(probs)] \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m-\u001b[39m probs\u001b[39m.\u001b[39msum()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X14sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m probs\u001b[39m.\u001b[39msqueeze()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X14sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m m \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mdistributions\u001b[39m.\u001b[39;49mCategorical(probs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X14sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m action \u001b[39m=\u001b[39m m\u001b[39m.\u001b[39msample()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X14sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m action\u001b[39m.\u001b[39mitem(), m\u001b[39m.\u001b[39mlog_prob(action)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/distributions/categorical.py:70\u001b[0m, in \u001b[0;36mCategorical.__init__\u001b[0;34m(self, probs, logits, validate_args)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_events \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m     67\u001b[0m batch_shape \u001b[39m=\u001b[39m (\n\u001b[1;32m     68\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_param\u001b[39m.\u001b[39mndimension() \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m torch\u001b[39m.\u001b[39mSize()\n\u001b[1;32m     69\u001b[0m )\n\u001b[0;32m---> 70\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(batch_shape, validate_args\u001b[39m=\u001b[39;49mvalidate_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/distributions/distribution.py:68\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[0;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[1;32m     66\u001b[0m         valid \u001b[39m=\u001b[39m constraint\u001b[39m.\u001b[39mcheck(value)\n\u001b[1;32m     67\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m valid\u001b[39m.\u001b[39mall():\n\u001b[0;32m---> 68\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m     69\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mExpected parameter \u001b[39m\u001b[39m{\u001b[39;00mparam\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     70\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(value)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m of shape \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtuple\u001b[39m(value\u001b[39m.\u001b[39mshape)\u001b[39m}\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     71\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mof distribution \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     72\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mto satisfy the constraint \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mrepr\u001b[39m(constraint)\u001b[39m}\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     73\u001b[0m                 \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbut found invalid values:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mvalue\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m \u001b[39msuper\u001b[39m()\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[0;31mValueError\u001b[0m: Expected parameter probs (Tensor of shape (2,)) of distribution Categorical(probs: torch.Size([2])) to satisfy the constraint Simplex(), but found invalid values:\ntensor([nan, nan], device='mps:0')"
     ]
    }
   ],
   "source": [
    "session.run(2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(27517.0, 27517)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.record_best_effort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and saving the new checkpoints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "torch.save(session.policy_net.state_dict(), \"float-16-good-policy.pt\")\n",
    "torch.save(session.value_net.state_dict(), \"float-16-good-value.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/b0kch01/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m quantize_dynamic(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     session\u001b[39m.\u001b[39mpolicy_net,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     {torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mLinear},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mqint8,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m session\u001b[39m.\u001b[39mquantized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m session\u001b[39m.\u001b[39;49mrun(\u001b[39m500\u001b[39;49m)\n",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(episodes):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     returns, std, steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mppo_step()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpisode \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode\u001b[39m}\u001b[39;00m\u001b[39m - Returns: \u001b[39m\u001b[39m{\u001b[39;00mreturns\u001b[39m}\u001b[39;00m\u001b[39m - Std: \u001b[39m\u001b[39m{\u001b[39;00mstd\u001b[39m}\u001b[39;00m\u001b[39m - Steps: \u001b[39m\u001b[39m{\u001b[39;00msteps\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m policy_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mmin(surrogate_loss, clipped_surrogate_loss)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m policy_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m value_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpow(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_net(batch_states) \u001b[39m-\u001b[39m batch_returns\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Load weights\n",
    "env = CartPoleQuantized(TYPE, DEVICE)\n",
    "session = PPOSession(env)\n",
    "session.policy_net.load_state_dict(torch.load(\"checkpoints/float-32-good-policy.pt\"))\n",
    "session.value_net.load_state_dict(torch.load(\"checkpoints/float-32-good-value.pt\"))\n",
    "\n",
    "session.policy_net.eval()\n",
    "session.value_net.eval()\n",
    "\n",
    "torch.backends.quantized.engine = \"qnnpack\"\n",
    "\n",
    "quantize_dynamic(\n",
    "    session.policy_net,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "session.quantized = True\n",
    "session.run(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15321.0, 15321)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#session.record_best_effort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
