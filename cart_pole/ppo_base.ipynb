{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Base Implementation\n",
    "This will be the baseline implementation for comparing with the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import LeakyReLU, ReLU, Linear, MSELoss, Sequential, Softmax, Dropout\n",
    "from torch.optim import Adam\n",
    "from torch.quantization import quantize_dynamic\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.99\n",
    "EPOCHS = 1\n",
    "CLIP_EPSILON = 0.2\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "TYPE = torch.bfloat16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(TYPE)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "# torch.backends.cpu.deterministic = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, extras: dict):\n",
    "        self.data = []\n",
    "        self.base_time = time.time()\n",
    "        self.extras = extras\n",
    "\n",
    "    def add(self, p_loss, v_loss, steps):\n",
    "        if len(self.data) == 0:\n",
    "            self.base_time = time.time()\n",
    "\n",
    "        time_elapsed = round(time.time() - self.base_time, 5)\n",
    "        self.data.append((time_elapsed, p_loss, v_loss, steps))\n",
    "\n",
    "    def df(self):\n",
    "        return pd.DataFrame(self.data, columns=[\"time\", \"p_loss\", \"v_loss\", \"steps\"])\n",
    "\n",
    "    def save(self):\n",
    "        path = \"logs/\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        os.mkdir(path)\n",
    "        pd.DataFrame([self.extras]).to_csv(path + \"/info.csv\")\n",
    "        pd.DataFrame(\n",
    "            self.data,\n",
    "            columns=[\"time\", \"p_loss\", \"v_loss\", \"steps\"]\n",
    "        ).to_csv(path + \"/data.csv\")\n",
    "\n",
    "\n",
    "logger = Logger({\n",
    "    \"seed\": SEED,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"gamma\": GAMMA,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"clip_epsilon\": CLIP_EPSILON,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"device\": DEVICE,\n",
    "    \"type\": TYPE\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleQuantized:\n",
    "    def __init__(self, dtype: torch.dtype, device: torch.device, record=True):\n",
    "        self.dtype = dtype\n",
    "        self.device = device\n",
    "\n",
    "        env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "\n",
    "        if record:\n",
    "            env = gym.wrappers.RecordVideo(\n",
    "                env, f\"training/{dtype}/\", episode_trigger=lambda x: x % 100 == 0 and x >= 30)\n",
    "            env.start_video_recorder()\n",
    "\n",
    "        env.reset(seed=SEED)\n",
    "\n",
    "        self.env = env\n",
    "\n",
    "    def reset(self):\n",
    "        return torch.tensor(self.env.reset(seed=SEED)[0], dtype=self.dtype, device=self.device)\n",
    "\n",
    "    def step(self, action):\n",
    "        state, reward, done, truncated, _ = self.env.step(action)\n",
    "        return torch.tensor(state, dtype=self.dtype, device=self.device), reward, done or truncated\n",
    "\n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "\n",
    "    @staticmethod\n",
    "    def env():\n",
    "        return gym.make('CartPole-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PolicyNetwork**:\n",
    "- Input: State\n",
    "- Output: Action distribution (0-1)\n",
    "- 2 Hidden layers with LeakyReLU activation\n",
    "\n",
    "**ValueNetwork**:\n",
    "- Input: State\n",
    "- Output: Value\n",
    "- 2 Hidden layers with LeakyReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolicyNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.model = Sequential(\n",
    "            Linear(input_dim, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, 2),\n",
    "            Softmax()\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.model(state)\n",
    "\n",
    "    def stochastic_action(self, state):\n",
    "        r\"\"\"Returns an action sampled from the policy network.\"\"\"\n",
    "        probs = self.forward(state).detach()\n",
    "        # adding floating point error to the maximum probability\n",
    "        probs[torch.argmax(probs)] += 1 - probs.sum()\n",
    "\n",
    "        probs.squeeze()  # quantized tensors have an extra dimension\n",
    "\n",
    "        m = torch.distributions.Categorical(probs)\n",
    "        action = m.sample()\n",
    "        return action.item(), m.log_prob(action)\n",
    "\n",
    "    def deterministic_action(self, state):\n",
    "        r\"\"\"Returns an action with the highest probability.\"\"\"\n",
    "\n",
    "        probs = self.forward(state).detach()\n",
    "        action = torch.argmax(probs)\n",
    "        return action.item()\n",
    "\n",
    "\n",
    "class ValueNetwork(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim) -> None:\n",
    "        super().__init__()\n",
    "        self.model = Sequential(\n",
    "            Linear(input_dim, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, hidden_dim),\n",
    "            ReLU(),\n",
    "            Linear(hidden_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.model(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "- 64 hidden nodes\n",
    "- Adam optimizer\n",
    "- MSE loss for value network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOSession:\n",
    "    def __init__(self, env: CartPoleQuantized):\n",
    "        self.env = env\n",
    "        self.episode = 0\n",
    "\n",
    "        _observation_size = CartPoleQuantized.env().observation_space.shape[0]\n",
    "\n",
    "        self.policy_net = PolicyNetwork(_observation_size, 64).to(DEVICE)\n",
    "        self.value_net = ValueNetwork(_observation_size, 64).to(DEVICE)\n",
    "\n",
    "        self.policy_optimizer = Adam(\n",
    "            self.policy_net.parameters(), lr=LEARNING_RATE, eps=1e-7)\n",
    "        self.value_optimizer = Adam(\n",
    "            self.value_net.parameters(), lr=LEARNING_RATE, eps=1e-7)\n",
    "        self.quantized = False\n",
    "\n",
    "    @staticmethod\n",
    "    def compute_returns(rewards):\n",
    "        returns = torch.zeros(len(rewards))\n",
    "        R = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = rewards[i] + GAMMA * R\n",
    "            returns[i] = R\n",
    "        return returns\n",
    "\n",
    "    def run(self, episodes):\n",
    "\n",
    "        def mean(l: list):\n",
    "            return round(sum(l)/len(l), 3)\n",
    "\n",
    "        self.policy_net.train()\n",
    "        self.value_net.train()\n",
    "\n",
    "        batch_returns = []\n",
    "        batch_std = []\n",
    "        batch_steps = []\n",
    "\n",
    "        for i in range(episodes):\n",
    "            self.episode += 1\n",
    "            returns, std, steps = self.ppo_step()\n",
    "\n",
    "            batch_returns.append(returns.item())\n",
    "            batch_std.append(std.item())\n",
    "            batch_steps.append(steps)\n",
    "\n",
    "            if self.episode % 5 == 0:\n",
    "                print(\n",
    "                    f\"Episode {self.episode} - Returns: {mean(batch_returns)} - Std: {mean(batch_std)} - Steps: {mean(batch_steps)}\")\n",
    "                batch_returns = []\n",
    "                batch_std = []\n",
    "                batch_steps = []\n",
    "\n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "\n",
    "    def ppo_step(self):\n",
    "        state = self.env.reset()\n",
    "\n",
    "        # capture entire episode\n",
    "        done, steps = False, 0\n",
    "        states, actions, log_probs_old, rewards = [], [], [], []\n",
    "\n",
    "        while not done:\n",
    "            if self.quantized:\n",
    "                state = state.unsqueeze(0)\n",
    "            action, log_prob = self.policy_net.stochastic_action(state)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "\n",
    "            log_probs_old.append(log_prob)\n",
    "            states.append(state.squeeze())\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "\n",
    "        # Convert to tensors\n",
    "        # Be sure to detach() the tensors from the graph as these are \"constants\"\n",
    "        states = torch.stack(states).detach().to(DEVICE)\n",
    "        actions = torch.tensor(actions).detach().to(DEVICE)\n",
    "        log_probs_old = torch.stack(log_probs_old).detach().to(DEVICE)\n",
    "\n",
    "        returns = self.compute_returns(rewards).detach().to(DEVICE)\n",
    "        values = self.value_net(states).detach().to(DEVICE)\n",
    "        advantages = returns - values.squeeze()\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            for i in range(0, len(states), BATCH_SIZE):\n",
    "                # Grab a batch of data\n",
    "                batch_states = states[i:i+BATCH_SIZE]\n",
    "                batch_actions = actions[i:i+BATCH_SIZE]\n",
    "                batch_log_probs_old = log_probs_old[i:i+BATCH_SIZE]\n",
    "                batch_advantages = advantages[i:i+BATCH_SIZE]\n",
    "                batch_returns = returns[i:i+BATCH_SIZE]\n",
    "\n",
    "                # Calculate new log probabilities\n",
    "                new_action_probs = self.policy_net(batch_states)\n",
    "                p1 = new_action_probs.gather(1, batch_actions.unsqueeze(-1))\n",
    "\n",
    "                if TYPE != torch.float32:\n",
    "                    new_log_probs = torch.log(torch.clamp(p1, min=1e-7))\n",
    "                else:\n",
    "                    new_log_probs = torch.log(p1)\n",
    "\n",
    "                # rho is the ratio between new and old log probabilities\n",
    "                ratio = (new_log_probs - batch_log_probs_old).exp()\n",
    "\n",
    "                # Calculate surrogate loss\n",
    "                surrogate_loss = ratio * batch_advantages\n",
    "                clipped_surrogate_loss = torch.clamp(\n",
    "                    ratio, 1-CLIP_EPSILON, 1+CLIP_EPSILON) * batch_advantages\n",
    "                policy_loss = -torch.min(surrogate_loss,\n",
    "                                         clipped_surrogate_loss).mean()\n",
    "\n",
    "                self.policy_optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                self.policy_optimizer.step()\n",
    "\n",
    "                # check for nan\n",
    "                if torch.isnan(policy_loss):\n",
    "                    print(\"NaN detected in policy loss\")\n",
    "                    return\n",
    "\n",
    "                value_loss = torch.pow(self.value_net(\n",
    "                    batch_states) - batch_returns.unsqueeze(-1), 2).mean()\n",
    "\n",
    "                self.value_optimizer.zero_grad()\n",
    "                value_loss.backward()\n",
    "                self.value_optimizer.step()\n",
    "\n",
    "                # check for nan\n",
    "                if torch.isnan(value_loss):\n",
    "                    print(\"NaN detected in value loss\")\n",
    "                    return\n",
    "\n",
    "        logger.add(policy_loss.item(), value_loss.item(), steps)\n",
    "        return (returns.mean(), returns.std(), steps)\n",
    "\n",
    "    def record_best_effort(self):\n",
    "        env = gym.make('CartPole-v1', render_mode='rgb_array',\n",
    "                       max_episode_steps=10000)\n",
    "        env = gym.wrappers.RecordVideo(env, \"tests\")\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(\n",
    "            state, dtype=TYPE, device=DEVICE, requires_grad=False)\n",
    "        env.start_video_recorder()\n",
    "\n",
    "        total_reward = 0\n",
    "        done, i = False, 0\n",
    "\n",
    "        while not done and not truncated:\n",
    "            if self.quantized:\n",
    "                state = state.unsqueeze(0)\n",
    "\n",
    "            action = self.policy_net.deterministic_action(state)\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            state = torch.tensor(\n",
    "                state, dtype=TYPE, device=DEVICE, requires_grad=False).unsqueeze(0)\n",
    "            total_reward += reward\n",
    "            i += 1\n",
    "\n",
    "        env.close()\n",
    "        return total_reward, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CartPoleQuantized(TYPE, DEVICE, record=False)\n",
    "session = PPOSession(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/b0kch01/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 - Returns: 7.731 - Std: 4.169 - Steps: 15.4\n",
      "Episode 10 - Returns: 9.812 - Std: 5.269 - Steps: 20.2\n",
      "Episode 15 - Returns: 9.131 - Std: 4.938 - Steps: 18.4\n",
      "Episode 20 - Returns: 9.106 - Std: 4.916 - Steps: 18.4\n",
      "Episode 25 - Returns: 12.537 - Std: 6.75 - Steps: 26.4\n",
      "Episode 30 - Returns: 8.137 - Std: 4.406 - Steps: 16.2\n",
      "Episode 35 - Returns: 10.7 - Std: 5.794 - Steps: 22.0\n",
      "Episode 40 - Returns: 9.894 - Std: 5.347 - Steps: 20.4\n",
      "Episode 45 - Returns: 10.594 - Std: 5.734 - Steps: 21.8\n",
      "Episode 50 - Returns: 18.744 - Std: 9.753 - Steps: 43.2\n",
      "Episode 55 - Returns: 9.681 - Std: 5.241 - Steps: 19.8\n",
      "Episode 60 - Returns: 10.512 - Std: 5.656 - Steps: 21.8\n",
      "Episode 65 - Returns: 8.856 - Std: 4.781 - Steps: 18.0\n",
      "Episode 70 - Returns: 11.269 - Std: 6.066 - Steps: 23.6\n",
      "Episode 75 - Returns: 11.312 - Std: 6.025 - Steps: 24.2\n",
      "Episode 80 - Returns: 11.656 - Std: 6.181 - Steps: 25.2\n",
      "Episode 85 - Returns: 8.9 - Std: 4.816 - Steps: 18.0\n",
      "Episode 90 - Returns: 9.531 - Std: 5.141 - Steps: 19.4\n",
      "Episode 95 - Returns: 14.637 - Std: 7.856 - Steps: 31.6\n",
      "Episode 100 - Returns: 9.438 - Std: 5.091 - Steps: 19.2\n",
      "Episode 105 - Returns: 11.4 - Std: 6.088 - Steps: 24.4\n",
      "Episode 110 - Returns: 6.713 - Std: 3.647 - Steps: 13.0\n",
      "Episode 115 - Returns: 9.325 - Std: 5.05 - Steps: 18.8\n",
      "Episode 120 - Returns: 12.8 - Std: 6.822 - Steps: 27.6\n",
      "Episode 125 - Returns: 8.438 - Std: 4.588 - Steps: 16.8\n",
      "Episode 130 - Returns: 9.162 - Std: 4.953 - Steps: 18.6\n",
      "Episode 135 - Returns: 9.838 - Std: 5.325 - Steps: 20.0\n",
      "Episode 140 - Returns: 10.231 - Std: 5.503 - Steps: 21.2\n",
      "Episode 145 - Returns: 14.625 - Std: 7.794 - Steps: 31.8\n",
      "Episode 150 - Returns: 11.012 - Std: 5.912 - Steps: 23.2\n",
      "Episode 155 - Returns: 11.537 - Std: 6.213 - Steps: 24.2\n",
      "Episode 160 - Returns: 11.762 - Std: 6.347 - Steps: 24.6\n",
      "Episode 165 - Returns: 11.625 - Std: 6.269 - Steps: 24.2\n",
      "Episode 170 - Returns: 8.969 - Std: 4.844 - Steps: 18.2\n",
      "Episode 175 - Returns: 12.95 - Std: 6.903 - Steps: 28.0\n",
      "Episode 180 - Returns: 10.581 - Std: 5.7 - Steps: 22.0\n",
      "Episode 185 - Returns: 13.912 - Std: 7.316 - Steps: 31.0\n",
      "Episode 190 - Returns: 11.45 - Std: 6.194 - Steps: 23.8\n",
      "Episode 195 - Returns: 11.7 - Std: 6.222 - Steps: 25.2\n",
      "Episode 200 - Returns: 10.95 - Std: 5.912 - Steps: 22.8\n",
      "Episode 205 - Returns: 12.762 - Std: 6.856 - Steps: 27.0\n",
      "Episode 210 - Returns: 6.719 - Std: 3.631 - Steps: 13.0\n",
      "Episode 215 - Returns: 12.213 - Std: 6.537 - Steps: 25.8\n",
      "Episode 220 - Returns: 11.713 - Std: 6.3 - Steps: 24.6\n",
      "Episode 225 - Returns: 11.231 - Std: 6.044 - Steps: 23.4\n",
      "Episode 230 - Returns: 9.838 - Std: 5.338 - Steps: 20.0\n",
      "Episode 235 - Returns: 8.144 - Std: 4.378 - Steps: 16.4\n",
      "Episode 240 - Returns: 11.012 - Std: 5.912 - Steps: 23.0\n",
      "Episode 245 - Returns: 11.225 - Std: 6.075 - Steps: 23.2\n",
      "Episode 250 - Returns: 8.588 - Std: 4.653 - Steps: 17.2\n",
      "Episode 255 - Returns: 10.95 - Std: 5.9 - Steps: 22.8\n",
      "Episode 260 - Returns: 16.788 - Std: 8.894 - Steps: 37.2\n",
      "Episode 265 - Returns: 11.875 - Std: 6.406 - Steps: 24.8\n",
      "Episode 270 - Returns: 9.031 - Std: 4.894 - Steps: 18.2\n",
      "Episode 275 - Returns: 8.3 - Std: 4.5 - Steps: 16.6\n",
      "Episode 280 - Returns: 10.356 - Std: 5.597 - Steps: 21.2\n",
      "Episode 285 - Returns: 11.125 - Std: 6.019 - Steps: 23.0\n",
      "Episode 290 - Returns: 10.975 - Std: 5.925 - Steps: 22.8\n",
      "Episode 295 - Returns: 7.263 - Std: 3.941 - Steps: 14.2\n",
      "Episode 300 - Returns: 10.162 - Std: 5.506 - Steps: 20.8\n",
      "Episode 305 - Returns: 13.325 - Std: 7.138 - Steps: 28.6\n",
      "Episode 310 - Returns: 12.225 - Std: 6.556 - Steps: 26.0\n",
      "Episode 315 - Returns: 15.387 - Std: 8.225 - Steps: 33.4\n",
      "Episode 320 - Returns: 9.675 - Std: 5.172 - Steps: 20.2\n",
      "Episode 325 - Returns: 12.8 - Std: 6.772 - Steps: 28.2\n",
      "Episode 330 - Returns: 11.181 - Std: 6.034 - Steps: 23.2\n",
      "Episode 335 - Returns: 11.95 - Std: 6.444 - Steps: 25.0\n",
      "Episode 340 - Returns: 10.762 - Std: 5.838 - Steps: 22.2\n",
      "Episode 345 - Returns: 12.981 - Std: 6.862 - Steps: 28.4\n",
      "Episode 350 - Returns: 12.762 - Std: 6.747 - Steps: 28.0\n",
      "Episode 355 - Returns: 8.481 - Std: 4.603 - Steps: 17.0\n",
      "Episode 360 - Returns: 7.812 - Std: 4.247 - Steps: 15.4\n",
      "Episode 365 - Returns: 11.6 - Std: 6.25 - Steps: 24.2\n",
      "Episode 370 - Returns: 14.8 - Std: 7.844 - Steps: 32.6\n",
      "Episode 375 - Returns: 9.806 - Std: 5.309 - Steps: 20.0\n",
      "Episode 380 - Returns: 8.75 - Std: 4.747 - Steps: 17.6\n",
      "Episode 385 - Returns: 11.012 - Std: 5.953 - Steps: 22.8\n",
      "Episode 390 - Returns: 8.144 - Std: 4.406 - Steps: 16.2\n",
      "Episode 395 - Returns: 11.969 - Std: 6.434 - Steps: 25.2\n",
      "Episode 400 - Returns: 10.006 - Std: 5.416 - Steps: 20.4\n",
      "Episode 405 - Returns: 7.7 - Std: 4.169 - Steps: 15.2\n",
      "Episode 410 - Returns: 9.656 - Std: 5.209 - Steps: 19.8\n",
      "Episode 415 - Returns: 12.113 - Std: 6.509 - Steps: 25.6\n",
      "Episode 420 - Returns: 11.075 - Std: 5.987 - Steps: 23.0\n",
      "Episode 425 - Returns: 7.263 - Std: 3.95 - Steps: 14.2\n",
      "Episode 430 - Returns: 8.925 - Std: 4.85 - Steps: 18.0\n",
      "Episode 435 - Returns: 9.2 - Std: 5.0 - Steps: 18.6\n",
      "Episode 440 - Returns: 12.744 - Std: 6.812 - Steps: 27.4\n",
      "Episode 445 - Returns: 13.85 - Std: 7.419 - Steps: 29.8\n",
      "Episode 450 - Returns: 11.625 - Std: 6.287 - Steps: 24.2\n",
      "Episode 455 - Returns: 10.525 - Std: 5.706 - Steps: 21.6\n",
      "Episode 460 - Returns: 10.7 - Std: 5.741 - Steps: 22.4\n",
      "Episode 465 - Returns: 12.325 - Std: 6.581 - Steps: 26.2\n",
      "Episode 470 - Returns: 7.888 - Std: 4.272 - Steps: 15.6\n",
      "Episode 475 - Returns: 8.412 - Std: 4.556 - Steps: 16.8\n",
      "Episode 480 - Returns: 10.356 - Std: 5.55 - Steps: 21.6\n",
      "Episode 485 - Returns: 12.675 - Std: 6.8 - Steps: 27.0\n",
      "Episode 490 - Returns: 14.137 - Std: 7.513 - Steps: 31.0\n",
      "Episode 495 - Returns: 12.275 - Std: 6.566 - Steps: 26.2\n",
      "Episode 500 - Returns: 8.556 - Std: 4.609 - Steps: 17.4\n",
      "Episode 505 - Returns: 10.225 - Std: 5.528 - Steps: 21.0\n",
      "Episode 510 - Returns: 8.838 - Std: 4.787 - Steps: 17.8\n",
      "Episode 515 - Returns: 8.6 - Std: 4.659 - Steps: 17.2\n",
      "Episode 520 - Returns: 8.244 - Std: 4.475 - Steps: 16.4\n",
      "Episode 525 - Returns: 9.625 - Std: 5.216 - Steps: 19.6\n",
      "Episode 530 - Returns: 9.281 - Std: 5.028 - Steps: 18.8\n",
      "Episode 535 - Returns: 10.137 - Std: 5.409 - Steps: 21.4\n",
      "Episode 540 - Returns: 8.244 - Std: 4.466 - Steps: 16.4\n",
      "Episode 545 - Returns: 12.838 - Std: 6.912 - Steps: 27.2\n",
      "Episode 550 - Returns: 10.544 - Std: 5.656 - Steps: 22.0\n",
      "Episode 555 - Returns: 10.656 - Std: 5.744 - Steps: 22.2\n",
      "Episode 560 - Returns: 9.331 - Std: 5.053 - Steps: 19.0\n",
      "Episode 565 - Returns: 9.475 - Std: 5.138 - Steps: 19.2\n",
      "Episode 570 - Returns: 13.206 - Std: 7.037 - Steps: 28.6\n",
      "Episode 575 - Returns: 7.531 - Std: 4.091 - Steps: 14.8\n",
      "Episode 580 - Returns: 8.85 - Std: 4.803 - Steps: 17.8\n",
      "Episode 585 - Returns: 9.144 - Std: 4.941 - Steps: 18.6\n",
      "Episode 590 - Returns: 15.562 - Std: 8.075 - Steps: 35.8\n",
      "Episode 595 - Returns: 10.713 - Std: 5.75 - Steps: 22.4\n",
      "Episode 600 - Returns: 10.175 - Std: 5.5 - Steps: 21.0\n",
      "Episode 605 - Returns: 9.037 - Std: 4.894 - Steps: 18.2\n",
      "Episode 610 - Returns: 7.95 - Std: 4.3 - Steps: 15.8\n",
      "Episode 615 - Returns: 8.694 - Std: 4.728 - Steps: 17.4\n",
      "Episode 620 - Returns: 10.325 - Std: 5.55 - Steps: 21.4\n",
      "Episode 625 - Returns: 9.8 - Std: 5.3 - Steps: 20.0\n",
      "Episode 630 - Returns: 11.3 - Std: 6.106 - Steps: 23.6\n",
      "Episode 635 - Returns: 9.463 - Std: 5.128 - Steps: 19.2\n",
      "Episode 640 - Returns: 11.075 - Std: 5.95 - Steps: 23.2\n",
      "Episode 645 - Returns: 9.375 - Std: 5.069 - Steps: 19.0\n",
      "Episode 650 - Returns: 10.787 - Std: 5.797 - Steps: 22.6\n",
      "Episode 655 - Returns: 10.044 - Std: 5.431 - Steps: 20.6\n",
      "Episode 660 - Returns: 12.425 - Std: 6.588 - Steps: 27.0\n",
      "Episode 665 - Returns: 7.987 - Std: 4.322 - Steps: 15.8\n",
      "Episode 670 - Returns: 17.375 - Std: 9.225 - Steps: 38.4\n",
      "Episode 675 - Returns: 11.356 - Std: 6.128 - Steps: 23.8\n",
      "Episode 680 - Returns: 7.362 - Std: 3.994 - Steps: 14.4\n",
      "Episode 685 - Returns: 15.688 - Std: 8.3 - Steps: 34.8\n",
      "Episode 690 - Returns: 11.062 - Std: 5.959 - Steps: 23.0\n",
      "Episode 695 - Returns: 9.537 - Std: 5.162 - Steps: 19.4\n",
      "Episode 700 - Returns: 11.037 - Std: 5.975 - Steps: 22.8\n",
      "Episode 705 - Returns: 9.588 - Std: 5.206 - Steps: 19.4\n",
      "Episode 710 - Returns: 8.781 - Std: 4.763 - Steps: 17.6\n",
      "Episode 715 - Returns: 9.925 - Std: 5.378 - Steps: 20.2\n",
      "Episode 720 - Returns: 7.731 - Std: 4.191 - Steps: 15.2\n",
      "Episode 725 - Returns: 13.312 - Std: 7.138 - Steps: 28.4\n",
      "Episode 730 - Returns: 9.644 - Std: 5.222 - Steps: 19.6\n",
      "Episode 735 - Returns: 11.544 - Std: 6.209 - Steps: 24.4\n",
      "Episode 740 - Returns: 13.081 - Std: 7.003 - Steps: 28.2\n",
      "Episode 745 - Returns: 8.906 - Std: 4.812 - Steps: 18.0\n",
      "Episode 750 - Returns: 9.625 - Std: 5.197 - Steps: 19.6\n",
      "Episode 755 - Returns: 10.3 - Std: 5.528 - Steps: 21.4\n",
      "Episode 760 - Returns: 7.169 - Std: 3.875 - Steps: 14.0\n",
      "Episode 765 - Returns: 7.781 - Std: 4.219 - Steps: 15.4\n",
      "Episode 770 - Returns: 13.35 - Std: 7.119 - Steps: 29.0\n",
      "Episode 775 - Returns: 10.044 - Std: 5.431 - Steps: 20.6\n",
      "Episode 780 - Returns: 10.619 - Std: 5.716 - Steps: 22.0\n",
      "Episode 785 - Returns: 12.925 - Std: 6.919 - Steps: 27.6\n",
      "Episode 790 - Returns: 15.15 - Std: 8.069 - Steps: 33.0\n",
      "Episode 795 - Returns: 13.875 - Std: 7.444 - Steps: 29.6\n",
      "Episode 800 - Returns: 8.906 - Std: 4.822 - Steps: 18.0\n",
      "Episode 805 - Returns: 7.95 - Std: 4.312 - Steps: 15.8\n",
      "Episode 810 - Returns: 13.25 - Std: 7.138 - Steps: 28.0\n",
      "Episode 815 - Returns: 9.725 - Std: 5.269 - Steps: 19.8\n",
      "Episode 820 - Returns: 12.656 - Std: 6.784 - Steps: 27.0\n",
      "Episode 825 - Returns: 12.762 - Std: 6.85 - Steps: 27.0\n",
      "Episode 830 - Returns: 10.875 - Std: 5.856 - Steps: 22.6\n",
      "Episode 835 - Returns: 15.812 - Std: 8.381 - Steps: 35.0\n",
      "Episode 840 - Returns: 16.325 - Std: 8.403 - Steps: 38.6\n",
      "Episode 845 - Returns: 11.225 - Std: 6.062 - Steps: 23.2\n",
      "Episode 850 - Returns: 8.669 - Std: 4.703 - Steps: 17.4\n",
      "Episode 855 - Returns: 11.556 - Std: 6.222 - Steps: 24.2\n",
      "Episode 860 - Returns: 13.775 - Std: 7.381 - Steps: 29.6\n",
      "Episode 865 - Returns: 11.838 - Std: 6.347 - Steps: 25.2\n",
      "Episode 870 - Returns: 9.037 - Std: 4.897 - Steps: 18.2\n",
      "Episode 875 - Returns: 10.262 - Std: 5.556 - Steps: 21.0\n",
      "Episode 880 - Returns: 10.331 - Std: 5.478 - Steps: 22.2\n",
      "Episode 885 - Returns: 8.5 - Std: 4.609 - Steps: 17.0\n",
      "Episode 890 - Returns: 10.05 - Std: 5.431 - Steps: 20.6\n",
      "Episode 895 - Returns: 10.469 - Std: 5.659 - Steps: 21.6\n",
      "Episode 900 - Returns: 9.5 - Std: 5.156 - Steps: 19.2\n",
      "Episode 905 - Returns: 7.975 - Std: 4.322 - Steps: 15.8\n",
      "Episode 910 - Returns: 9.481 - Std: 5.147 - Steps: 19.2\n",
      "Episode 915 - Returns: 11.775 - Std: 6.362 - Steps: 24.6\n",
      "Episode 920 - Returns: 9.894 - Std: 5.356 - Steps: 20.2\n",
      "Episode 925 - Returns: 14.125 - Std: 7.566 - Steps: 30.4\n",
      "Episode 930 - Returns: 9.75 - Std: 5.259 - Steps: 20.0\n",
      "Episode 935 - Returns: 7.844 - Std: 4.244 - Steps: 15.6\n",
      "Episode 940 - Returns: 14.419 - Std: 7.619 - Steps: 32.0\n",
      "Episode 945 - Returns: 8.575 - Std: 4.644 - Steps: 17.2\n",
      "Episode 950 - Returns: 15.581 - Std: 8.172 - Steps: 35.4\n",
      "Episode 955 - Returns: 11.438 - Std: 6.15 - Steps: 24.0\n",
      "Episode 960 - Returns: 12.475 - Std: 6.719 - Steps: 26.4\n",
      "Episode 965 - Returns: 12.662 - Std: 6.831 - Steps: 26.6\n",
      "Episode 970 - Returns: 11.213 - Std: 6.047 - Steps: 23.4\n",
      "Episode 975 - Returns: 9.869 - Std: 5.328 - Steps: 20.2\n",
      "Episode 980 - Returns: 11.356 - Std: 5.947 - Steps: 25.2\n",
      "Episode 985 - Returns: 10.231 - Std: 5.525 - Steps: 21.0\n",
      "Episode 990 - Returns: 8.606 - Std: 4.653 - Steps: 17.4\n",
      "Episode 995 - Returns: 10.056 - Std: 5.428 - Steps: 20.6\n",
      "Episode 1000 - Returns: 11.325 - Std: 6.125 - Steps: 23.6\n"
     ]
    }
   ],
   "source": [
    "session.run(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.record_best_effort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and saving the new checkpoints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "torch.save(session.policy_net.state_dict(), \"float-16-good-policycpu.pt\")\n",
    "torch.save(session.value_net.state_dict(), \"float-16-good-valuecpu.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/b0kch01/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m quantize_dynamic(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     session\u001b[39m.\u001b[39mpolicy_net,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     {torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mLinear},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mqint8,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m session\u001b[39m.\u001b[39mquantized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m session\u001b[39m.\u001b[39;49mrun(\u001b[39m500\u001b[39;49m)\n",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(episodes):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     returns, std, steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mppo_step()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpisode \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode\u001b[39m}\u001b[39;00m\u001b[39m - Returns: \u001b[39m\u001b[39m{\u001b[39;00mreturns\u001b[39m}\u001b[39;00m\u001b[39m - Std: \u001b[39m\u001b[39m{\u001b[39;00mstd\u001b[39m}\u001b[39;00m\u001b[39m - Steps: \u001b[39m\u001b[39m{\u001b[39;00msteps\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m policy_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mmin(surrogate_loss, clipped_surrogate_loss)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m policy_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m value_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpow(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_net(batch_states) \u001b[39m-\u001b[39m batch_returns\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Load weights\n",
    "env = CartPoleQuantized(TYPE, DEVICE)\n",
    "session = PPOSession(env)\n",
    "session.policy_net.load_state_dict(\n",
    "    torch.load(\"checkpoints/float-32-good-policy.pt\"))\n",
    "session.value_net.load_state_dict(\n",
    "    torch.load(\"checkpoints/float-32-good-value.pt\"))\n",
    "\n",
    "session.policy_net.eval()\n",
    "session.value_net.eval()\n",
    "\n",
    "torch.backends.quantized.engine = \"qnnpack\"\n",
    "\n",
    "quantize_dynamic(\n",
    "    session.policy_net,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "session.quantized = True\n",
    "session.run(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15321.0, 15321)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# session.record_best_effort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
