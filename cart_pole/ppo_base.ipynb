{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Base Implementation\n",
    "This will be the baseline implementation for comparing with the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import LeakyReLU, ReLU, Linear, MSELoss, Sequential, Softmax, Dropout\n",
    "from torch.optim import Adam\n",
    "from torch.quantization import quantize_dynamic\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.99\n",
    "EPOCHS = 1\n",
    "CLIP_EPSILON = 0.2\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "TYPE = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(TYPE)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "# torch.backends.cpu.deterministic = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, extras: dict):\n",
    "        self.data = []\n",
    "        self.base_time = time.time()\n",
    "        self.extras = extras\n",
    "        \n",
    "    def add(self, p_loss, v_loss, steps):\n",
    "        if len(self.data) == 0:\n",
    "            self.base_time = time.time()\n",
    "        \n",
    "        time_elapsed = round(time.time() - self.base_time, 5)\n",
    "        self.data.append((time_elapsed, p_loss, v_loss, steps))\n",
    "    \n",
    "    def df(self):\n",
    "        return pd.DataFrame(self.data, columns=[\"time\", \"p_loss\", \"v_loss\", \"steps\"])\n",
    "    \n",
    "    def save(self):\n",
    "        path = \"logs/\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        os.mkdir(path)\n",
    "        pd.DataFrame([self.extras]).to_csv(path + \"/info.csv\")\n",
    "        pd.DataFrame(\n",
    "            self.data, \n",
    "            columns=[\"time\", \"p_loss\", \"v_loss\", \"steps\"]\n",
    "        ).to_csv(path + \"/data.csv\")\n",
    "\n",
    "logger = Logger({\n",
    "    \"seed\": SEED,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"gamma\": GAMMA,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"clip_epsilon\": CLIP_EPSILON,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"device\": DEVICE,\n",
    "    \"type\": TYPE\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleQuantized:\n",
    "  def __init__(self, dtype: torch.dtype, device: torch.device):\n",
    "    self.dtype = dtype\n",
    "    self.device = device\n",
    "    \n",
    "    env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "    env = gym.wrappers.RecordVideo(env, f\"training/{dtype}/\", episode_trigger=lambda x: x % 100 == 0 and x >= 30)\n",
    "    env.reset()\n",
    "    env.start_video_recorder()\n",
    "    \n",
    "    self.env = env\n",
    "  \n",
    "  def reset(self):\n",
    "    return torch.tensor(self.env.reset()[0], dtype=self.dtype, device=self.device)\n",
    "  \n",
    "  def step(self, action):\n",
    "    state, reward, done, truncated, _ = self.env.step(action)\n",
    "    return torch.tensor(state, dtype=self.dtype, device=self.device), reward, done or truncated\n",
    "  \n",
    "  def close(self):\n",
    "    self.env.close()\n",
    "  \n",
    "  @staticmethod\n",
    "  def env():\n",
    "    return gym.make('CartPole-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PolicyNetwork**:\n",
    "- Input: State\n",
    "- Output: Action distribution (0-1)\n",
    "- 2 Hidden layers with LeakyReLU activation\n",
    "\n",
    "**ValueNetwork**:\n",
    "- Input: State\n",
    "- Output: Value\n",
    "- 2 Hidden layers with LeakyReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolicyNetwork(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim):\n",
    "    super().__init__()\n",
    "    self.model = Sequential(\n",
    "      Linear(input_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, 2),\n",
    "      Softmax()\n",
    "    )\n",
    "\n",
    "  def forward(self, state): \n",
    "    return self.model(state)\n",
    " \n",
    "  def stochastic_action(self, state):\n",
    "    r\"\"\"Returns an action sampled from the policy network.\"\"\"\n",
    "    \n",
    "    probs = self.forward(state).detach()\n",
    "    # adding floating point error to the maximum probability\n",
    "    probs[torch.argmax(probs)] += 1 - probs.sum()\n",
    "    \n",
    "    probs.squeeze() # quantized tensors have an extra dimension\n",
    "    \n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action.item(), m.log_prob(action)\n",
    "  \n",
    "  def deterministic_action(self, state):\n",
    "    r\"\"\"Returns an action with the highest probability.\"\"\"\n",
    "    \n",
    "    probs = self.forward(state).detach()\n",
    "    action = torch.argmax(probs)\n",
    "    return action.item()\n",
    "\n",
    "  \n",
    "class ValueNetwork(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim) -> None:\n",
    "    super().__init__()\n",
    "    self.model = Sequential(\n",
    "      Linear(input_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, 1)\n",
    "    )\n",
    "  \n",
    "  def forward(self, state):\n",
    "    return self.model(state)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "- 64 hidden nodes\n",
    "- Adam optimizer\n",
    "- MSE loss for value network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOSession:\n",
    "    def __init__(self, env: CartPoleQuantized):\n",
    "        self.env = env\n",
    "        self.episode = 0\n",
    "        \n",
    "        _observation_size = CartPoleQuantized.env().observation_space.shape[0]\n",
    "\n",
    "        self.policy_net = PolicyNetwork(_observation_size, 64).to(DEVICE)\n",
    "        self.value_net  = ValueNetwork(_observation_size, 64).to(DEVICE)\n",
    "\n",
    "        self.policy_optimizer = Adam(self.policy_net.parameters(), lr=LEARNING_RATE, eps=1e-7)\n",
    "        self.value_optimizer  = Adam(self.value_net.parameters(), lr=LEARNING_RATE, eps=1e-7)\n",
    "        self.quantized = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_returns(rewards):\n",
    "        returns = torch.zeros(len(rewards))\n",
    "        R = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = rewards[i] + GAMMA * R\n",
    "            returns[i] = R\n",
    "        return returns\n",
    "    \n",
    "    def run(self, episodes):\n",
    "        self.policy_net.train()\n",
    "        self.value_net.train()\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            self.episode += 1\n",
    "            returns, std, steps = self.ppo_step()\n",
    "            \n",
    "            if self.episode % 5 == 0:\n",
    "                print(f\"Episode {self.episode} - Returns: {returns} - Std: {std} - Steps: {steps}\")\n",
    "\n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "    \n",
    "    def ppo_step(self):\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        # capture entire episode\n",
    "        done, steps = False, 0\n",
    "        states, actions, log_probs_old, rewards = [], [], [], []\n",
    "        \n",
    "        while not done:\n",
    "            if self.quantized:\n",
    "                state = state.unsqueeze(0)\n",
    "            action, log_prob = self.policy_net.stochastic_action(state)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "\n",
    "            log_probs_old.append(log_prob)\n",
    "            states.append(state.squeeze())\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        \n",
    "        # Convert to tensors\n",
    "        # Be sure to detach() the tensors from the graph as these are \"constants\"\n",
    "        states = torch.stack(states).detach().to(DEVICE)\n",
    "        actions = torch.tensor(actions).detach().to(DEVICE)\n",
    "        log_probs_old = torch.stack(log_probs_old).detach().to(DEVICE)\n",
    "        \n",
    "        returns = self.compute_returns(rewards).detach().to(DEVICE)\n",
    "        values = self.value_net(states).detach().to(DEVICE)\n",
    "        advantages = returns - values.squeeze()\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            for i in range(0, len(states), BATCH_SIZE):\n",
    "                # Grab a batch of data\n",
    "                batch_states = states[i:i+BATCH_SIZE]\n",
    "                batch_actions = actions[i:i+BATCH_SIZE]\n",
    "                batch_log_probs_old = log_probs_old[i:i+BATCH_SIZE]\n",
    "                batch_advantages = advantages[i:i+BATCH_SIZE]\n",
    "                batch_returns = returns[i:i+BATCH_SIZE]\n",
    "\n",
    "                # Calculate new log probabilities\n",
    "                new_action_probs = self.policy_net(batch_states)\n",
    "                new_log_probs = torch.log(new_action_probs.gather(1, batch_actions.unsqueeze(-1)))\n",
    "\n",
    "                # rho is the ratio between new and old log probabilities\n",
    "                ratio = (new_log_probs - batch_log_probs_old).exp()\n",
    "\n",
    "                # Calculate surrogate loss\n",
    "                surrogate_loss = ratio * batch_advantages\n",
    "                clipped_surrogate_loss = torch.clamp(ratio, 1-CLIP_EPSILON, 1+CLIP_EPSILON) * batch_advantages\n",
    "                policy_loss = -torch.min(surrogate_loss, clipped_surrogate_loss).mean()\n",
    "                \n",
    "                self.policy_optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                self.policy_optimizer.step()\n",
    "\n",
    "                # check for nan\n",
    "                if torch.isnan(policy_loss):\n",
    "                    print(\"NaN detected in policy loss\")\n",
    "                    return\n",
    "\n",
    "                value_loss = torch.pow(self.value_net(\n",
    "                    batch_states) - batch_returns.unsqueeze(-1), 2).mean()\n",
    "\n",
    "                self.value_optimizer.zero_grad()\n",
    "                value_loss.backward()\n",
    "                self.value_optimizer.step()\n",
    "\n",
    "                # check for nan\n",
    "                if torch.isnan(value_loss):\n",
    "                    print(\"NaN detected in value loss\")\n",
    "                    return\n",
    "        \n",
    "        logger.add(policy_loss.item(), value_loss.item(), steps)\n",
    "        return (returns.mean(), returns.std(), steps)\n",
    "    \n",
    "    def record_best_effort(self):\n",
    "        env = gym.make('CartPole-v1', render_mode='rgb_array', max_episode_steps=10000)\n",
    "        env = gym.wrappers.RecordVideo(env, \"tests\")\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=TYPE, device=DEVICE, requires_grad=False)\n",
    "        env.start_video_recorder()\n",
    "\n",
    "        total_reward = 0\n",
    "        done, i = False, 0\n",
    "        \n",
    "        while not done and not truncated:\n",
    "            if self.quantized:\n",
    "                state = state.unsqueeze(0)\n",
    "            \n",
    "            action = self.policy_net.deterministic_action(state)\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            state = torch.tensor(state, dtype=TYPE, device=DEVICE, requires_grad=False).unsqueeze(0)\n",
    "            total_reward += reward\n",
    "            i += 1\n",
    "\n",
    "        env.close()\n",
    "        return total_reward, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "env = CartPoleQuantized(TYPE, DEVICE)\n",
    "session = PPOSession(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/torch/nn/modules/container.py:204: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  input = module(input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 - Returns: 10.733765602111816 - Std: 5.823357582092285 - Steps: 22\n",
      "Episode 10 - Returns: 14.101123809814453 - Std: 7.5943074226379395 - Steps: 30\n",
      "Episode 15 - Returns: 10.300280570983887 - Std: 5.5916337966918945 - Steps: 21\n",
      "Episode 20 - Returns: 12.439506530761719 - Std: 6.7269415855407715 - Steps: 26\n",
      "Episode 25 - Returns: 8.089495658874512 - Std: 4.396872043609619 - Steps: 16\n",
      "Episode 30 - Returns: 5.804428577423096 - Std: 3.1395034790039062 - Steps: 11\n",
      "Episode 35 - Returns: 6.267518997192383 - Std: 3.39615535736084 - Steps: 12\n",
      "Episode 40 - Returns: 6.267518997192383 - Std: 3.39615535736084 - Steps: 12\n",
      "Episode 45 - Returns: 13.275748252868652 - Std: 7.165063858032227 - Steps: 28\n",
      "Episode 50 - Returns: 6.267518997192383 - Std: 3.39615535736084 - Steps: 12\n",
      "Episode 55 - Returns: 5.804428577423096 - Std: 3.1395034790039062 - Steps: 11\n",
      "Episode 60 - Returns: 17.68506622314453 - Std: 9.420500755310059 - Steps: 39\n",
      "Episode 65 - Returns: 7.184539794921875 - Std: 3.901630401611328 - Steps: 14\n",
      "Episode 70 - Returns: 22.150283813476562 - Std: 11.606053352355957 - Steps: 51\n",
      "Episode 75 - Returns: 9.424704551696777 - Std: 5.121031284332275 - Steps: 19\n",
      "Episode 80 - Returns: 14.915790557861328 - Std: 8.01484203338623 - Steps: 32\n",
      "Episode 85 - Returns: 16.513635635375977 - Std: 8.830445289611816 - Steps: 36\n",
      "Episode 90 - Returns: 29.809003829956055 - Std: 15.101871490478516 - Steps: 74\n",
      "Episode 95 - Returns: 12.439506530761719 - Std: 6.7269415855407715 - Steps: 26\n",
      "Episode 100 - Returns: 46.18396759033203 - Std: 21.29043960571289 - Steps: 138\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-100.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-100.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-100.mp4\n",
      "Episode 105 - Returns: 11.164410591125488 - Std: 6.052726745605469 - Steps: 23\n",
      "Episode 110 - Returns: 30.117435455322266 - Std: 15.235461235046387 - Steps: 75\n",
      "Episode 115 - Returns: 14.509784698486328 - Std: 7.805652618408203 - Steps: 31\n",
      "Episode 120 - Returns: 9.863934516906738 - Std: 5.357532978057861 - Steps: 20\n",
      "Episode 125 - Returns: 21.4298095703125 - Std: 11.260408401489258 - Steps: 49\n",
      "Episode 130 - Returns: 13.689783096313477 - Std: 7.38078498840332 - Steps: 29\n",
      "Episode 135 - Returns: 46.393192291259766 - Std: 21.356096267700195 - Steps: 139\n",
      "Episode 140 - Returns: 14.101123809814453 - Std: 7.5943074226379395 - Steps: 30\n",
      "Episode 145 - Returns: 8.537507057189941 - Std: 4.640725612640381 - Steps: 17\n",
      "Episode 150 - Returns: 47.21794509887695 - Std: 21.61115264892578 - Steps: 143\n",
      "Episode 155 - Returns: 36.17048263549805 - Std: 17.734111785888672 - Steps: 96\n",
      "Episode 160 - Returns: 22.5070743560791 - Std: 11.776198387145996 - Steps: 52\n",
      "Episode 165 - Returns: 24.257217407226562 - Std: 12.600817680358887 - Steps: 57\n",
      "Episode 170 - Returns: 18.453466415405273 - Std: 9.80385684967041 - Steps: 41\n",
      "Episode 175 - Returns: 14.101123809814453 - Std: 7.5943074226379395 - Steps: 30\n",
      "Episode 180 - Returns: 36.70710372924805 - Std: 17.94379234313965 - Steps: 98\n",
      "Episode 185 - Returns: 22.5070743560791 - Std: 11.776198387145996 - Steps: 52\n",
      "Episode 190 - Returns: 19.96080780029297 - Std: 10.547213554382324 - Steps: 45\n",
      "Episode 195 - Returns: 24.941802978515625 - Std: 12.918790817260742 - Steps: 59\n",
      "Episode 200 - Returns: 49.58015823364258 - Std: 22.307462692260742 - Steps: 155\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-200.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-200.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-200.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 205 - Returns: 50.145572662353516 - Std: 22.466360092163086 - Steps: 158\n",
      "Episode 210 - Returns: 35.07728576660156 - Std: 17.300790786743164 - Steps: 92\n",
      "Episode 215 - Returns: 23.213842391967773 - Std: 12.11121654510498 - Steps: 54\n",
      "Episode 220 - Returns: 19.96080780029297 - Std: 10.547213554382324 - Steps: 45\n",
      "Episode 225 - Returns: 33.95683670043945 - Std: 16.848249435424805 - Steps: 88\n",
      "Episode 230 - Returns: 32.223331451416016 - Std: 16.131746292114258 - Steps: 82\n",
      "Episode 235 - Returns: 52.659725189208984 - Std: 23.13473892211914 - Steps: 172\n",
      "Episode 240 - Returns: 40.040138244628906 - Std: 19.19992446899414 - Steps: 111\n",
      "Episode 245 - Returns: 39.79297637939453 - Std: 19.109582901000977 - Steps: 110\n",
      "Episode 250 - Returns: 40.530006408691406 - Std: 19.37761688232422 - Steps: 113\n",
      "Episode 255 - Returns: 41.25383377075195 - Std: 19.636817932128906 - Steps: 116\n",
      "Episode 260 - Returns: 27.594907760620117 - Std: 14.125996589660645 - Steps: 67\n",
      "Episode 265 - Returns: 68.26155853271484 - Std: 25.582008361816406 - Steps: 296\n",
      "Episode 270 - Returns: 37.7608642578125 - Std: 18.34963035583496 - Steps: 102\n",
      "Episode 275 - Returns: 55.613868713378906 - Std: 23.83574867248535 - Steps: 190\n",
      "Episode 280 - Returns: 56.834999084472656 - Std: 24.09709930419922 - Steps: 198\n",
      "Episode 285 - Returns: 60.455909729003906 - Std: 24.766027450561523 - Steps: 224\n",
      "Episode 290 - Returns: 18.453466415405273 - Std: 9.80385684967041 - Steps: 41\n",
      "Episode 295 - Returns: 59.79829788208008 - Std: 24.65687370300293 - Steps: 219\n",
      "Episode 300 - Returns: 65.76427459716797 - Std: 25.42462921142578 - Steps: 270\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-300.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-300.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-300.mp4\n",
      "Episode 305 - Returns: 71.98341369628906 - Std: 25.60456657409668 - Steps: 342\n",
      "Episode 310 - Returns: 57.86044692993164 - Std: 24.303028106689453 - Steps: 205\n",
      "Episode 315 - Returns: 60.96955490112305 - Std: 24.847301483154297 - Steps: 228\n",
      "Episode 320 - Returns: 71.91067504882812 - Std: 25.60679054260254 - Steps: 341\n",
      "Episode 325 - Returns: 47.62316131591797 - Std: 21.73423957824707 - Steps: 145\n",
      "Episode 330 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 335 - Returns: 44.24451446533203 - Std: 20.663883209228516 - Steps: 129\n",
      "Episode 340 - Returns: 66.269775390625 - Std: 25.4649715423584 - Steps: 275\n",
      "Episode 345 - Returns: 59.25944137573242 - Std: 24.56324005126953 - Steps: 215\n",
      "Episode 350 - Returns: 74.88227081298828 - Std: 25.419845581054688 - Steps: 386\n",
      "Episode 355 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 360 - Returns: 56.383365631103516 - Std: 24.00244903564453 - Steps: 195\n",
      "Episode 365 - Returns: 77.47988891601562 - Std: 25.078493118286133 - Steps: 434\n",
      "Episode 370 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 375 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 380 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 385 - Returns: 60.19496536254883 - Std: 24.7233943939209 - Steps: 222\n",
      "Episode 390 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 395 - Returns: 69.14083099365234 - Std: 25.61136817932129 - Steps: 306\n",
      "Episode 400 - Returns: 68.52980041503906 - Std: 25.59246063232422 - Steps: 299\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-400.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-400.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-400.mp4\n",
      "Episode 405 - Returns: 71.83761596679688 - Std: 25.60890769958496 - Steps: 340\n",
      "Episode 410 - Returns: 73.8925552368164 - Std: 25.504718780517578 - Steps: 370\n",
      "Episode 415 - Returns: 80.21537017822266 - Std: 24.509235382080078 - Steps: 497\n",
      "Episode 420 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 425 - Returns: 71.91067504882812 - Std: 25.60679054260254 - Steps: 341\n",
      "Episode 430 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 435 - Returns: 68.79418182373047 - Std: 25.601484298706055 - Steps: 302\n",
      "Episode 440 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 445 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 450 - Returns: 65.03495788574219 - Std: 25.359161376953125 - Steps: 263\n",
      "Episode 455 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 460 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 465 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 470 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 475 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 480 - Returns: 64.71441650390625 - Std: 25.327730178833008 - Steps: 260\n",
      "Episode 485 - Returns: 56.685279846191406 - Std: 24.06598663330078 - Steps: 197\n",
      "Episode 490 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 495 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 500 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-500.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-500.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-500.mp4\n",
      "Episode 505 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 510 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 515 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 520 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 525 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 530 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 535 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 540 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 545 - Returns: 63.26738357543945 - Std: 25.166269302368164 - Steps: 247\n",
      "Episode 550 - Returns: 71.54199981689453 - Std: 25.616334915161133 - Steps: 336\n",
      "Episode 555 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 560 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 565 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 570 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 575 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 580 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 585 - Returns: 79.08065795898438 - Std: 24.773744583129883 - Steps: 469\n",
      "Episode 590 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 595 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n",
      "Episode 600 - Returns: 80.3301010131836 - Std: 24.480113983154297 - Steps: 500\n"
     ]
    }
   ],
   "source": [
    "session.run(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-7dc58757ea2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_best_effort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-175-0d247f1c975d>\u001b[0m in \u001b[0;36mrecord_best_effort\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterministic_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-cee4c537c51d>\u001b[0m in \u001b[0;36mdeterministic_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34mr\"\"\"Returns an action with the highest probability.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-cee4c537c51d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mstochastic_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1259\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "session.record_best_effort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and saving the new checkpoints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "torch.save(session.policy_net.state_dict(), \"float-16-good-policy.pt\")\n",
    "torch.save(session.value_net.state_dict(), \"float-16-good-value.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/b0kch01/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m quantize_dynamic(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     session\u001b[39m.\u001b[39mpolicy_net,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     {torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mLinear},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mqint8,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m session\u001b[39m.\u001b[39mquantized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m session\u001b[39m.\u001b[39;49mrun(\u001b[39m500\u001b[39;49m)\n",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(episodes):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     returns, std, steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mppo_step()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpisode \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode\u001b[39m}\u001b[39;00m\u001b[39m - Returns: \u001b[39m\u001b[39m{\u001b[39;00mreturns\u001b[39m}\u001b[39;00m\u001b[39m - Std: \u001b[39m\u001b[39m{\u001b[39;00mstd\u001b[39m}\u001b[39;00m\u001b[39m - Steps: \u001b[39m\u001b[39m{\u001b[39;00msteps\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m policy_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mmin(surrogate_loss, clipped_surrogate_loss)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m policy_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m value_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpow(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_net(batch_states) \u001b[39m-\u001b[39m batch_returns\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Load weights\n",
    "env = CartPoleQuantized(TYPE, DEVICE)\n",
    "session = PPOSession(env)\n",
    "session.policy_net.load_state_dict(torch.load(\"checkpoints/float-32-good-policy.pt\"))\n",
    "session.value_net.load_state_dict(torch.load(\"checkpoints/float-32-good-value.pt\"))\n",
    "\n",
    "session.policy_net.eval()\n",
    "session.value_net.eval()\n",
    "\n",
    "torch.backends.quantized.engine = \"qnnpack\"\n",
    "\n",
    "quantize_dynamic(\n",
    "    session.policy_net,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "session.quantized = True\n",
    "session.run(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15321.0, 15321)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#session.record_best_effort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
