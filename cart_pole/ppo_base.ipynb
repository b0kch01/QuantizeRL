{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Base Implementation\n",
    "This will be the baseline implementation for comparing with the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import LeakyReLU, ReLU, Linear, MSELoss, Sequential, Softmax, Dropout\n",
    "from torch.optim import Adam\n",
    "from torch.quantization import quantize_dynamic\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.99\n",
    "EPOCHS = 1\n",
    "CLIP_EPSILON = 0.2\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "TYPE = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(TYPE)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "# torch.backends.cpu.deterministic = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, extras: dict):\n",
    "        self.data = []\n",
    "        self.base_time = time.time()\n",
    "        self.extras = extras\n",
    "        \n",
    "    def add(self, p_loss, v_loss, steps):\n",
    "        if len(self.data) == 0:\n",
    "            self.base_time = time.time()\n",
    "        \n",
    "        time_elapsed = round(time.time() - self.base_time, 5)\n",
    "        self.data.append((time_elapsed, p_loss, v_loss, steps))\n",
    "    \n",
    "    def df(self):\n",
    "        return pd.DataFrame(self.data, columns=[\"time\", \"p_loss\", \"v_loss\", \"steps\"])\n",
    "    \n",
    "    def save(self):\n",
    "        path = \"logs/\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        os.mkdir(path)\n",
    "        pd.DataFrame([self.extras]).to_csv(path + \"/info.csv\")\n",
    "        pd.DataFrame(\n",
    "            self.data, \n",
    "            columns=[\"time\", \"p_loss\", \"v_loss\", \"steps\"]\n",
    "        ).to_csv(path + \"/data.csv\")\n",
    "\n",
    "logger = Logger({\n",
    "    \"seed\": SEED,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"gamma\": GAMMA,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"clip_epsilon\": CLIP_EPSILON,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"device\": DEVICE,\n",
    "    \"type\": TYPE\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleQuantized:\n",
    "  def __init__(self, dtype: torch.dtype, device: torch.device):\n",
    "    self.dtype = dtype\n",
    "    self.device = device\n",
    "    \n",
    "    env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "    env = gym.wrappers.RecordVideo(env, f\"training/{dtype}/\", episode_trigger=lambda x: x % 100 == 0 and x >= 30)\n",
    "    env.reset()\n",
    "    env.start_video_recorder()\n",
    "    \n",
    "    self.env = env\n",
    "  \n",
    "  def reset(self):\n",
    "    return torch.tensor(self.env.reset()[0], dtype=self.dtype, device=self.device)\n",
    "  \n",
    "  def step(self, action):\n",
    "    state, reward, done, truncated, _ = self.env.step(action)\n",
    "    return torch.tensor(state, dtype=self.dtype, device=self.device), reward, done or truncated\n",
    "  \n",
    "  def close(self):\n",
    "    self.env.close()\n",
    "  \n",
    "  @staticmethod\n",
    "  def env():\n",
    "    return gym.make('CartPole-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PolicyNetwork**:\n",
    "- Input: State\n",
    "- Output: Action distribution (0-1)\n",
    "- 2 Hidden layers with LeakyReLU activation\n",
    "\n",
    "**ValueNetwork**:\n",
    "- Input: State\n",
    "- Output: Value\n",
    "- 2 Hidden layers with LeakyReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolicyNetwork(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim):\n",
    "    super().__init__()\n",
    "    self.model = Sequential(\n",
    "      Linear(input_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, 2),\n",
    "      Softmax()\n",
    "    )\n",
    "\n",
    "  def forward(self, state): \n",
    "    return self.model(state)\n",
    " \n",
    "  def stochastic_action(self, state):\n",
    "    r\"\"\"Returns an action sampled from the policy network.\"\"\"\n",
    "    \n",
    "    probs = self.forward(state).detach()\n",
    "    # adding floating point error to the maximum probability\n",
    "    probs[torch.argmax(probs)] += 1 - probs.sum()\n",
    "    \n",
    "    probs.squeeze() # quantized tensors have an extra dimension\n",
    "    \n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action.item(), m.log_prob(action)\n",
    "  \n",
    "  def deterministic_action(self, state):\n",
    "    r\"\"\"Returns an action with the highest probability.\"\"\"\n",
    "    \n",
    "    probs = self.forward(state).detach()\n",
    "    action = torch.argmax(probs)\n",
    "    return action.item()\n",
    "\n",
    "  \n",
    "class ValueNetwork(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim) -> None:\n",
    "    super().__init__()\n",
    "    self.model = Sequential(\n",
    "      Linear(input_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, 1)\n",
    "    )\n",
    "  \n",
    "  def forward(self, state):\n",
    "    return self.model(state)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "- 64 hidden nodes\n",
    "- Adam optimizer\n",
    "- MSE loss for value network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOSession:\n",
    "    def __init__(self, env: CartPoleQuantized):\n",
    "        self.env = env\n",
    "        self.episode = 0\n",
    "        \n",
    "        _observation_size = CartPoleQuantized.env().observation_space.shape[0]\n",
    "\n",
    "        self.policy_net = PolicyNetwork(_observation_size, 64).to(DEVICE)\n",
    "        self.value_net  = ValueNetwork(_observation_size, 64).to(DEVICE)\n",
    "\n",
    "        self.policy_optimizer = Adam(self.policy_net.parameters(), lr=LEARNING_RATE, eps=1e-7)\n",
    "        self.value_optimizer  = Adam(self.value_net.parameters(), lr=LEARNING_RATE, eps=1e-7)\n",
    "        self.quantized = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_returns(rewards):\n",
    "        returns = torch.zeros(len(rewards))\n",
    "        R = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = rewards[i] + GAMMA * R\n",
    "            returns[i] = R\n",
    "        return returns\n",
    "    \n",
    "    def run(self, episodes):\n",
    "        self.policy_net.train()\n",
    "        self.value_net.train()\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            self.episode += 1\n",
    "            returns, std, steps = self.ppo_step()\n",
    "            \n",
    "            if self.episode % 5 == 0:\n",
    "                print(f\"Episode {self.episode} - Returns: {returns} - Std: {std} - Steps: {steps}\")\n",
    "\n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "    \n",
    "    def ppo_step(self):\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        # capture entire episode\n",
    "        done, steps = False, 0\n",
    "        states, actions, log_probs_old, rewards = [], [], [], []\n",
    "        \n",
    "        while not done:\n",
    "            if self.quantized:\n",
    "                state = state.unsqueeze(0)\n",
    "            action, log_prob = self.policy_net.stochastic_action(state)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "\n",
    "            log_probs_old.append(log_prob)\n",
    "            states.append(state.squeeze())\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        \n",
    "        # Convert to tensors\n",
    "        # Be sure to detach() the tensors from the graph as these are \"constants\"\n",
    "        states = torch.stack(states).detach().to(DEVICE)\n",
    "        actions = torch.tensor(actions).detach().to(DEVICE)\n",
    "        log_probs_old = torch.stack(log_probs_old).detach().to(DEVICE)\n",
    "        \n",
    "        returns = self.compute_returns(rewards).detach().to(DEVICE)\n",
    "        values = self.value_net(states).detach().to(DEVICE)\n",
    "        advantages = returns - values.squeeze()\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            for i in range(0, len(states), BATCH_SIZE):\n",
    "                # Grab a batch of data\n",
    "                batch_states = states[i:i+BATCH_SIZE]\n",
    "                batch_actions = actions[i:i+BATCH_SIZE]\n",
    "                batch_log_probs_old = log_probs_old[i:i+BATCH_SIZE]\n",
    "                batch_advantages = advantages[i:i+BATCH_SIZE]\n",
    "                batch_returns = returns[i:i+BATCH_SIZE]\n",
    "\n",
    "                # Calculate new log probabilities\n",
    "                new_action_probs = self.policy_net(batch_states)\n",
    "                new_log_probs = torch.log(new_action_probs.gather(1, batch_actions.unsqueeze(-1)))\n",
    "\n",
    "                # rho is the ratio between new and old log probabilities\n",
    "                ratio = (new_log_probs - batch_log_probs_old).exp()\n",
    "\n",
    "                # Calculate surrogate loss\n",
    "                surrogate_loss = ratio * batch_advantages\n",
    "                clipped_surrogate_loss = torch.clamp(ratio, 1-CLIP_EPSILON, 1+CLIP_EPSILON) * batch_advantages\n",
    "                policy_loss = -torch.min(surrogate_loss, clipped_surrogate_loss).mean()\n",
    "                \n",
    "                self.policy_optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                self.policy_optimizer.step()\n",
    "\n",
    "                # check for nan\n",
    "                if torch.isnan(policy_loss):\n",
    "                    print(\"NaN detected in policy loss\")\n",
    "                    return\n",
    "\n",
    "                value_loss = torch.pow(self.value_net(\n",
    "                    batch_states) - batch_returns.unsqueeze(-1), 2).mean()\n",
    "\n",
    "                self.value_optimizer.zero_grad()\n",
    "                value_loss.backward()\n",
    "                self.value_optimizer.step()\n",
    "\n",
    "                # check for nan\n",
    "                if torch.isnan(value_loss):\n",
    "                    print(\"NaN detected in value loss\")\n",
    "                    return\n",
    "        \n",
    "        logger.add(policy_loss.item(), value_loss.item(), steps)\n",
    "        return (returns.mean(), returns.std(), steps)\n",
    "    \n",
    "    def record_best_effort(self):\n",
    "        env = gym.make('CartPole-v1', render_mode='rgb_array', max_episode_steps=10000)\n",
    "        env = gym.wrappers.RecordVideo(env, \"tests\")\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=TYPE, device=DEVICE, requires_grad=False)\n",
    "        env.start_video_recorder()\n",
    "\n",
    "        total_reward = 0\n",
    "        done, i = False, 0\n",
    "        \n",
    "        while not done and not truncated:\n",
    "            if self.quantized:\n",
    "                state = state.unsqueeze(0)\n",
    "            \n",
    "            action = self.policy_net.deterministic_action(state)\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            state = torch.tensor(state, dtype=TYPE, device=DEVICE, requires_grad=False).unsqueeze(0)\n",
    "            total_reward += reward\n",
    "            i += 1\n",
    "\n",
    "        env.close()\n",
    "        return total_reward, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CartPoleQuantized(TYPE, DEVICE)\n",
    "session = PPOSession(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-0.mp4\n",
      "Episode 5 - Returns: 8.984375 - Std: 4.8828125 - Steps: 18\n",
      "Episode 10 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 15 - Returns: 8.0859375 - Std: 4.39453125 - Steps: 16\n",
      "Episode 20 - Returns: 13.6875 - Std: 7.3828125 - Steps: 29\n",
      "Episode 25 - Returns: 4.8671875 - Std: 2.619140625 - Steps: 9\n",
      "Episode 30 - Returns: 22.859375 - Std: 11.9453125 - Steps: 53\n",
      "Episode 35 - Returns: 8.0859375 - Std: 4.39453125 - Steps: 16\n",
      "Episode 40 - Returns: 6.26953125 - Std: 3.396484375 - Steps: 12\n",
      "Episode 45 - Returns: 7.640625 - Std: 4.1484375 - Steps: 15\n",
      "Episode 50 - Returns: 21.796875 - Std: 11.4375 - Steps: 50\n",
      "Episode 55 - Returns: 8.984375 - Std: 4.8828125 - Steps: 18\n",
      "Episode 60 - Returns: 13.6875 - Std: 7.3828125 - Steps: 29\n",
      "Episode 65 - Returns: 19.96875 - Std: 10.546875 - Steps: 45\n",
      "Episode 70 - Returns: 19.96875 - Std: 10.546875 - Steps: 45\n",
      "Episode 75 - Returns: 7.640625 - Std: 4.1484375 - Steps: 15\n",
      "Episode 80 - Returns: 22.515625 - Std: 11.78125 - Steps: 52\n",
      "Episode 85 - Returns: 25.625 - Std: 13.234375 - Steps: 61\n",
      "Episode 90 - Returns: 11.59375 - Std: 6.28125 - Steps: 24\n",
      "Episode 95 - Returns: 7.640625 - Std: 4.1484375 - Steps: 15\n",
      "Episode 100 - Returns: 13.6875 - Std: 7.3828125 - Steps: 29\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-100.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-100.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-100.mp4\n",
      "Episode 105 - Returns: 36.96875 - Std: 18.046875 - Steps: 99\n",
      "Episode 110 - Returns: 9.421875 - Std: 5.12109375 - Steps: 19\n",
      "Episode 115 - Returns: 15.71875 - Std: 8.4296875 - Steps: 34\n",
      "Episode 120 - Returns: 21.796875 - Std: 11.4375 - Steps: 50\n",
      "Episode 125 - Returns: 16.125 - Std: 8.6328125 - Steps: 35\n",
      "Episode 130 - Returns: 35.34375 - Std: 17.40625 - Steps: 93\n",
      "Episode 135 - Returns: 4.3984375 - Std: 2.353515625 - Steps: 8\n",
      "Episode 140 - Returns: 10.734375 - Std: 5.82421875 - Steps: 22\n",
      "Episode 145 - Returns: 17.6875 - Std: 9.421875 - Steps: 39\n",
      "Episode 150 - Returns: 18.453125 - Std: 9.8046875 - Steps: 41\n",
      "Episode 155 - Returns: 5.8046875 - Std: 3.140625 - Steps: 11\n",
      "Episode 160 - Returns: 12.4375 - Std: 6.7265625 - Steps: 26\n",
      "Episode 165 - Returns: 38.28125 - Std: 18.546875 - Steps: 104\n",
      "Episode 170 - Returns: 32.8125 - Std: 16.375 - Steps: 84\n",
      "Episode 175 - Returns: 34.25 - Std: 16.96875 - Steps: 89\n",
      "Episode 180 - Returns: 6.7265625 - Std: 3.650390625 - Steps: 13\n",
      "Episode 185 - Returns: 26.953125 - Std: 13.8359375 - Steps: 65\n",
      "Episode 190 - Returns: 16.125 - Std: 8.6328125 - Steps: 35\n",
      "Episode 195 - Returns: 20.703125 - Std: 10.90625 - Steps: 47\n",
      "Episode 200 - Returns: 15.71875 - Std: 8.4296875 - Steps: 34\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-200.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-200.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-200.mp4\n",
      "Episode 205 - Returns: 19.59375 - Std: 10.3671875 - Steps: 44\n",
      "Episode 210 - Returns: 31.03125 - Std: 15.625 - Steps: 78\n",
      "Episode 215 - Returns: 27.265625 - Std: 13.984375 - Steps: 66\n",
      "Episode 220 - Returns: 45.125 - Std: 20.953125 - Steps: 133\n",
      "Episode 225 - Returns: 32.21875 - Std: 16.125 - Steps: 82\n",
      "Episode 230 - Returns: 24.609375 - Std: 12.7578125 - Steps: 58\n",
      "Episode 235 - Returns: 11.59375 - Std: 6.28125 - Steps: 24\n",
      "Episode 240 - Returns: 23.90625 - Std: 12.4375 - Steps: 56\n",
      "Episode 245 - Returns: 25.953125 - Std: 13.3828125 - Steps: 62\n",
      "Episode 250 - Returns: 26.28125 - Std: 13.5390625 - Steps: 63\n",
      "Episode 255 - Returns: 41.0 - Std: 19.546875 - Steps: 115\n",
      "Episode 260 - Returns: 39.78125 - Std: 19.109375 - Steps: 110\n",
      "Episode 265 - Returns: 33.6875 - Std: 16.734375 - Steps: 87\n",
      "Episode 270 - Returns: 21.4375 - Std: 11.2578125 - Steps: 49\n",
      "Episode 275 - Returns: 30.125 - Std: 15.234375 - Steps: 75\n",
      "Episode 280 - Returns: 32.8125 - Std: 16.375 - Steps: 84\n",
      "Episode 285 - Returns: 43.8125 - Std: 20.515625 - Steps: 127\n",
      "Episode 290 - Returns: 32.8125 - Std: 16.375 - Steps: 84\n",
      "Episode 295 - Returns: 33.96875 - Std: 16.84375 - Steps: 88\n",
      "Episode 300 - Returns: 36.96875 - Std: 18.046875 - Steps: 99\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-300.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-300.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-300.mp4\n",
      "Episode 305 - Returns: 25.625 - Std: 13.234375 - Steps: 61\n",
      "Episode 310 - Returns: 47.625 - Std: 21.734375 - Steps: 145\n",
      "Episode 315 - Returns: 29.8125 - Std: 15.1015625 - Steps: 74\n",
      "Episode 320 - Returns: 31.921875 - Std: 16.0 - Steps: 81\n",
      "Episode 325 - Returns: 56.375 - Std: 24.0 - Steps: 195\n",
      "Episode 330 - Returns: 27.921875 - Std: 14.2734375 - Steps: 68\n",
      "Episode 335 - Returns: 47.40625 - Std: 21.671875 - Steps: 144\n",
      "Episode 340 - Returns: 35.90625 - Std: 17.625 - Steps: 95\n",
      "Episode 345 - Returns: 65.25 - Std: 25.375 - Steps: 265\n",
      "Episode 350 - Returns: 31.03125 - Std: 15.625 - Steps: 78\n",
      "Episode 355 - Returns: 50.53125 - Std: 22.5625 - Steps: 160\n",
      "Episode 360 - Returns: 33.96875 - Std: 16.84375 - Steps: 88\n",
      "Episode 365 - Returns: 40.53125 - Std: 19.375 - Steps: 113\n",
      "Episode 370 - Returns: 41.25 - Std: 19.640625 - Steps: 116\n",
      "Episode 375 - Returns: 55.9375 - Std: 23.90625 - Steps: 192\n",
      "Episode 380 - Returns: 47.40625 - Std: 21.671875 - Steps: 144\n",
      "Episode 385 - Returns: 49.96875 - Std: 22.40625 - Steps: 157\n",
      "Episode 390 - Returns: 62.21875 - Std: 25.03125 - Steps: 238\n",
      "Episode 395 - Returns: 36.15625 - Std: 17.734375 - Steps: 96\n",
      "Episode 400 - Returns: 62.3125 - Std: 25.046875 - Steps: 239\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-400.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-400.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-400.mp4\n",
      "Episode 405 - Returns: 60.46875 - Std: 24.765625 - Steps: 224\n",
      "Episode 410 - Returns: 23.5625 - Std: 12.2734375 - Steps: 55\n",
      "Episode 415 - Returns: 47.625 - Std: 21.734375 - Steps: 145\n",
      "Episode 420 - Returns: 63.71875 - Std: 25.21875 - Steps: 251\n",
      "Episode 425 - Returns: 67.0625 - Std: 25.515625 - Steps: 283\n",
      "Episode 430 - Returns: 61.84375 - Std: 24.96875 - Steps: 235\n",
      "Episode 435 - Returns: 41.0 - Std: 19.546875 - Steps: 115\n",
      "Episode 440 - Returns: 57.71875 - Std: 24.28125 - Steps: 204\n",
      "Episode 445 - Returns: 45.75 - Std: 21.15625 - Steps: 136\n",
      "Episode 450 - Returns: 62.8125 - Std: 25.109375 - Steps: 243\n",
      "Episode 455 - Returns: 67.6875 - Std: 25.5625 - Steps: 290\n",
      "Episode 460 - Returns: 67.875 - Std: 25.5625 - Steps: 292\n",
      "Episode 465 - Returns: 80.3125 - Std: 24.484375 - Steps: 500\n",
      "Episode 470 - Returns: 51.78125 - Std: 22.90625 - Steps: 167\n",
      "Episode 475 - Returns: 80.3125 - Std: 24.484375 - Steps: 500\n",
      "Episode 480 - Returns: 56.96875 - Std: 24.125 - Steps: 199\n",
      "Episode 485 - Returns: 79.0625 - Std: 24.78125 - Steps: 469\n",
      "Episode 490 - Returns: 60.84375 - Std: 24.828125 - Steps: 227\n",
      "Episode 495 - Returns: 56.21875 - Std: 23.96875 - Steps: 194\n",
      "Episode 500 - Returns: 79.0625 - Std: 24.78125 - Steps: 468\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-500.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-500.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/training/torch.float16/rl-video-episode-500.mp4\n",
      "Episode 505 - Returns: 80.3125 - Std: 24.484375 - Steps: 500\n",
      "Episode 510 - Returns: 80.3125 - Std: 24.484375 - Steps: 500\n",
      "Episode 515 - Returns: 80.3125 - Std: 24.484375 - Steps: 500\n",
      "Episode 520 - Returns: 80.3125 - Std: 24.484375 - Steps: 500\n",
      "Episode 525 - Returns: 80.3125 - Std: 24.484375 - Steps: 500\n",
      "Episode 530 - Returns: 80.3125 - Std: 24.484375 - Steps: 500\n",
      "Episode 535 - Returns: 80.3125 - Std: 24.484375 - Steps: 500\n",
      "Episode 540 - Returns: 11.59375 - Std: 6.28125 - Steps: 24\n",
      "Episode 545 - Returns: 80.3125 - Std: 24.484375 - Steps: 500\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-177-60b8093b89de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-175-0d247f1c975d>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m             \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-175-0d247f1c975d>\u001b[0m in \u001b[0;36mppo_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mpolicy_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    486\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    487\u001b[0m             )\n\u001b[0;32m--> 488\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    489\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         )\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "session.run(2500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-180-7dc58757ea2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_best_effort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-175-0d247f1c975d>\u001b[0m in \u001b[0;36mrecord_best_effort\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeterministic_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTYPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequires_grad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-cee4c537c51d>\u001b[0m in \u001b[0;36mdeterministic_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0;34mr\"\"\"Returns an action with the highest probability.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m     \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-174-cee4c537c51d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mstochastic_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m'_parameters'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m             \u001b[0m_parameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__dict__\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_parameters'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1259\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0m_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "session.record_best_effort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and saving the new checkpoints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "torch.save(session.policy_net.state_dict(), \"float-16-good-policy.pt\")\n",
    "torch.save(session.value_net.state_dict(), \"float-16-good-value.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/b0kch01/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m quantize_dynamic(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     session\u001b[39m.\u001b[39mpolicy_net,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     {torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mLinear},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mqint8,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m session\u001b[39m.\u001b[39mquantized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m session\u001b[39m.\u001b[39;49mrun(\u001b[39m500\u001b[39;49m)\n",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(episodes):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     returns, std, steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mppo_step()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpisode \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode\u001b[39m}\u001b[39;00m\u001b[39m - Returns: \u001b[39m\u001b[39m{\u001b[39;00mreturns\u001b[39m}\u001b[39;00m\u001b[39m - Std: \u001b[39m\u001b[39m{\u001b[39;00mstd\u001b[39m}\u001b[39;00m\u001b[39m - Steps: \u001b[39m\u001b[39m{\u001b[39;00msteps\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m policy_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mmin(surrogate_loss, clipped_surrogate_loss)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m policy_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m value_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpow(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_net(batch_states) \u001b[39m-\u001b[39m batch_returns\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Load weights\n",
    "env = CartPoleQuantized(TYPE, DEVICE)\n",
    "session = PPOSession(env)\n",
    "session.policy_net.load_state_dict(torch.load(\"checkpoints/float-32-good-policy.pt\"))\n",
    "session.value_net.load_state_dict(torch.load(\"checkpoints/float-32-good-value.pt\"))\n",
    "\n",
    "session.policy_net.eval()\n",
    "session.value_net.eval()\n",
    "\n",
    "torch.backends.quantized.engine = \"qnnpack\"\n",
    "\n",
    "quantize_dynamic(\n",
    "    session.policy_net,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "session.quantized = True\n",
    "session.run(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15321.0, 15321)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#session.record_best_effort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
