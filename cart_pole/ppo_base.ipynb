{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Base Implementation\n",
    "This will be the baseline implementation for comparing with the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.99\n",
    "EPOCHS = 10\n",
    "CLIP_EPSILON = 0.2\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear, Dropout, LeakyReLU, Softmax, Sequential, Sigmoid\n",
    "from torch.optim import Adam\n",
    "from torch.nn import MSELoss\n",
    "\n",
    "import gym\n",
    "\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v1')\n",
    "device = torch.device(\"mps\")\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PolicyNetwork**:\n",
    "- Input: State\n",
    "- Output: Action distribution (0-1)\n",
    "- 2 Hidden layers with LeakyReLU activation\n",
    "\n",
    "**ValueNetwork**:\n",
    "- Input: State\n",
    "- Output: Value\n",
    "- 2 Hidden layers with LeakyReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyNetwork(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim):\n",
    "    super(PolicyNetwork, self).__init__()\n",
    "    self.model = Sequential(\n",
    "      Linear(input_dim, hidden_dim),\n",
    "      LeakyReLU(),\n",
    "      Linear(hidden_dim, hidden_dim),\n",
    "      LeakyReLU(),\n",
    "      Linear(hidden_dim, 1),\n",
    "      Sigmoid()\n",
    "    )\n",
    "    \n",
    "  def forward(self, state):\n",
    "    return self.model(state)\n",
    " \n",
    " \n",
    "  def stochastic_action(self, state):\n",
    "    r\"\"\"Returns an action sampled from the policy network.\"\"\"\n",
    "    \n",
    "    state = torch.from_numpy(state).unsqueeze(0).to(device)\n",
    "    probs = self.forward(state).cpu()\n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action.item(), m.log_prob(action)\n",
    "\n",
    "  \n",
    "class ValueNetwork(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim) -> None:\n",
    "    super(ValueNetwork, self).__init__()\n",
    "    self.model = Sequential(\n",
    "      Linear(input_dim, hidden_dim),\n",
    "      LeakyReLU(),\n",
    "      Linear(hidden_dim, hidden_dim),\n",
    "      LeakyReLU(),\n",
    "      Linear(hidden_dim, 1)\n",
    "    )\n",
    "  \n",
    "  def forward(self, state):\n",
    "    return self.model(state)\n",
    "  \n",
    "  def value(self, state):\n",
    "    state = torch.from_numpy(state).unsqueeze(0).to(device)\n",
    "    return self.forward(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "_observation_size = env.observation_space.shape[0]\n",
    "\n",
    "policy_net = PolicyNetwork(_observation_size, 64).to(device)\n",
    "value_net  = ValueNetwork(_observation_size, 64).to(device)\n",
    "\n",
    "policy_optimizer = Adam(policy_net.parameters(), lr=LEARNING_RATE)\n",
    "value_optimizer  = Adam(value_net.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards):\n",
    "  returns = [0]*len(rewards)\n",
    "  R = 0\n",
    "  for i in reversed(range(len(rewards))):\n",
    "    R = rewards[i] + GAMMA * R\n",
    "    returns[i] = R\n",
    "  return torch.tensor(returns)"
   ]
  },
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-m1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
