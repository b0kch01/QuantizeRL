{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO Base Implementation\n",
    "This will be the baseline implementation for comparing with the other methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch.nn import LeakyReLU, ReLU, Linear, MSELoss, Sequential, Softmax, Dropout\n",
    "from torch.optim import Adam\n",
    "from torch.quantization import quantize_dynamic\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "LEARNING_RATE = 1e-4\n",
    "GAMMA = 0.99\n",
    "EPOCHS = 1\n",
    "CLIP_EPSILON = 0.2\n",
    "BATCH_SIZE = 1\n",
    "\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "TYPE = torch.float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_default_dtype(TYPE)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "# torch.backends.cpu.deterministic = True\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "class Logger:\n",
    "    def __init__(self, extras: dict):\n",
    "        self.data = []\n",
    "        self.base_time = time.time()\n",
    "        self.extras = extras\n",
    "        \n",
    "    def add(self, p_loss, v_loss, steps):\n",
    "        if len(self.data) == 0:\n",
    "            self.base_time = time.time()\n",
    "        \n",
    "        time_elapsed = round(time.time() - self.base_time, 5)\n",
    "        self.data.append((time_elapsed, p_loss, v_loss, steps))\n",
    "    \n",
    "    def df(self):\n",
    "        return pd.DataFrame(self.data, columns=[\"time\", \"p_loss\", \"v_loss\", \"steps\"])\n",
    "    \n",
    "    def save(self):\n",
    "        path = \"logs/\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "        os.mkdir(path)\n",
    "        pd.DataFrame([self.extras]).to_csv(path + \"/info.csv\")\n",
    "        pd.DataFrame(\n",
    "            self.data, \n",
    "            columns=[\"time\", \"p_loss\", \"v_loss\", \"steps\"]\n",
    "        ).to_csv(path + \"/data.csv\")\n",
    "\n",
    "logger = Logger({\n",
    "    \"seed\": SEED,\n",
    "    \"learning_rate\": LEARNING_RATE,\n",
    "    \"gamma\": GAMMA,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"clip_epsilon\": CLIP_EPSILON,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"device\": DEVICE,\n",
    "    \"type\": TYPE\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CartPoleQuantized:\n",
    "  def __init__(self, dtype: torch.dtype, device: torch.device, record=True):\n",
    "    self.dtype = dtype\n",
    "    self.device = device\n",
    "    \n",
    "    env = gym.make('CartPole-v1', render_mode='rgb_array')\n",
    "    \n",
    "    if record:\n",
    "        env = gym.wrappers.RecordVideo(env, f\"training/{dtype}/\", episode_trigger=lambda x: x % 100 == 0 and x >= 30)\n",
    "        env.start_video_recorder()\n",
    "    \n",
    "    env.reset()\n",
    "    \n",
    "    self.env = env\n",
    "  \n",
    "  def reset(self):\n",
    "    return torch.tensor(self.env.reset()[0], dtype=self.dtype, device=self.device)\n",
    "  \n",
    "  def step(self, action):\n",
    "    state, reward, done, truncated, _ = self.env.step(action)\n",
    "    return torch.tensor(state, dtype=self.dtype, device=self.device), reward, done or truncated\n",
    "  \n",
    "  def close(self):\n",
    "    self.env.close()\n",
    "  \n",
    "  @staticmethod\n",
    "  def env():\n",
    "    return gym.make('CartPole-v1', render_mode='rgb_array')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PolicyNetwork**:\n",
    "- Input: State\n",
    "- Output: Action distribution (0-1)\n",
    "- 2 Hidden layers with LeakyReLU activation\n",
    "\n",
    "**ValueNetwork**:\n",
    "- Input: State\n",
    "- Output: Value\n",
    "- 2 Hidden layers with LeakyReLU activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class PolicyNetwork(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim):\n",
    "    super().__init__()\n",
    "    self.model = Sequential(\n",
    "      Linear(input_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, 2),\n",
    "      Softmax()\n",
    "    )\n",
    "\n",
    "  def forward(self, state): \n",
    "    return self.model(state)\n",
    " \n",
    "  def stochastic_action(self, state):\n",
    "    r\"\"\"Returns an action sampled from the policy network.\"\"\"\n",
    "    \n",
    "    probs = self.forward(state).detach()\n",
    "    # adding floating point error to the maximum probability\n",
    "    probs[torch.argmax(probs)] += 1 - probs.sum()\n",
    "    \n",
    "    probs.squeeze() # quantized tensors have an extra dimension\n",
    "    \n",
    "    m = torch.distributions.Categorical(probs)\n",
    "    action = m.sample()\n",
    "    return action.item(), m.log_prob(action)\n",
    "  \n",
    "  def deterministic_action(self, state):\n",
    "    r\"\"\"Returns an action with the highest probability.\"\"\"\n",
    "    \n",
    "    probs = self.forward(state).detach()\n",
    "    action = torch.argmax(probs)\n",
    "    return action.item()\n",
    "\n",
    "  \n",
    "class ValueNetwork(torch.nn.Module):\n",
    "  def __init__(self, input_dim, hidden_dim) -> None:\n",
    "    super().__init__()\n",
    "    self.model = Sequential(\n",
    "      Linear(input_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, hidden_dim),\n",
    "      ReLU(),\n",
    "      Linear(hidden_dim, 1)\n",
    "    )\n",
    "  \n",
    "  def forward(self, state):\n",
    "    return self.model(state)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "- 64 hidden nodes\n",
    "- Adam optimizer\n",
    "- MSE loss for value network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPOSession:\n",
    "    def __init__(self, env: CartPoleQuantized):\n",
    "        self.env = env\n",
    "        self.episode = 0\n",
    "        \n",
    "        _observation_size = CartPoleQuantized.env().observation_space.shape[0]\n",
    "\n",
    "        self.policy_net = PolicyNetwork(_observation_size, 64).to(DEVICE)\n",
    "        self.value_net  = ValueNetwork(_observation_size, 64).to(DEVICE)\n",
    "\n",
    "        self.policy_optimizer = Adam(self.policy_net.parameters(), lr=LEARNING_RATE, eps=1e-7)\n",
    "        self.value_optimizer  = Adam(self.value_net.parameters(), lr=LEARNING_RATE, eps=1e-7)\n",
    "        self.quantized = False\n",
    "    \n",
    "    @staticmethod\n",
    "    def compute_returns(rewards):\n",
    "        returns = torch.zeros(len(rewards))\n",
    "        R = 0\n",
    "        for i in reversed(range(len(rewards))):\n",
    "            R = rewards[i] + GAMMA * R\n",
    "            returns[i] = R\n",
    "        return returns\n",
    "    \n",
    "    def run(self, episodes):\n",
    "        \n",
    "        def mean(l: list):\n",
    "            return round(sum(l)/len(l), 3)\n",
    "            \n",
    "        self.policy_net.train()\n",
    "        self.value_net.train()\n",
    "        \n",
    "        batch_returns = []\n",
    "        batch_std = []\n",
    "        batch_steps = []\n",
    "        \n",
    "        for i in range(episodes):\n",
    "            self.episode += 1\n",
    "            returns, std, steps = self.ppo_step()\n",
    "\n",
    "            batch_returns.append(returns.item())\n",
    "            batch_std.append(std.item())\n",
    "            batch_steps.append(steps)\n",
    "            \n",
    "            if self.episode % 50 == 0:\n",
    "                print(f\"Episode {self.episode} - Returns: {mean(batch_returns)} - Std: {mean(batch_std)} - Steps: {mean(batch_steps)}\")\n",
    "                batch_returns = []\n",
    "                batch_std = []\n",
    "                batch_steps = []\n",
    "\n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "    \n",
    "    def ppo_step(self):\n",
    "        state = self.env.reset()\n",
    "        \n",
    "        # capture entire episode\n",
    "        done, steps = False, 0\n",
    "        states, actions, log_probs_old, rewards = [], [], [], []\n",
    "        \n",
    "        while not done:\n",
    "            if self.quantized:\n",
    "                state = state.unsqueeze(0)\n",
    "            action, log_prob = self.policy_net.stochastic_action(state)\n",
    "            next_state, reward, done = self.env.step(action)\n",
    "\n",
    "            log_probs_old.append(log_prob)\n",
    "            states.append(state.squeeze())\n",
    "            actions.append(action)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        \n",
    "        # Convert to tensors\n",
    "        # Be sure to detach() the tensors from the graph as these are \"constants\"\n",
    "        states = torch.stack(states).detach().to(DEVICE)\n",
    "        actions = torch.tensor(actions).detach().to(DEVICE)\n",
    "        log_probs_old = torch.stack(log_probs_old).detach().to(DEVICE)\n",
    "        \n",
    "        returns = self.compute_returns(rewards).detach().to(DEVICE)\n",
    "        values = self.value_net(states).detach().to(DEVICE)\n",
    "        advantages = returns - values.squeeze()\n",
    "\n",
    "        for _ in range(EPOCHS):\n",
    "            for i in range(0, len(states), BATCH_SIZE):\n",
    "                # Grab a batch of data\n",
    "                batch_states = states[i:i+BATCH_SIZE]\n",
    "                batch_actions = actions[i:i+BATCH_SIZE]\n",
    "                batch_log_probs_old = log_probs_old[i:i+BATCH_SIZE]\n",
    "                batch_advantages = advantages[i:i+BATCH_SIZE]\n",
    "                batch_returns = returns[i:i+BATCH_SIZE]\n",
    "\n",
    "                # Calculate new log probabilities\n",
    "                new_action_probs = self.policy_net(batch_states)\n",
    "                new_log_probs = torch.log(new_action_probs.gather(1, batch_actions.unsqueeze(-1)))\n",
    "\n",
    "                # rho is the ratio between new and old log probabilities\n",
    "                ratio = (new_log_probs - batch_log_probs_old).exp()\n",
    "\n",
    "                # Calculate surrogate loss\n",
    "                surrogate_loss = ratio * batch_advantages\n",
    "                clipped_surrogate_loss = torch.clamp(ratio, 1-CLIP_EPSILON, 1+CLIP_EPSILON) * batch_advantages\n",
    "                policy_loss = -torch.min(surrogate_loss, clipped_surrogate_loss).mean()\n",
    "                \n",
    "                self.policy_optimizer.zero_grad()\n",
    "                policy_loss.backward()\n",
    "                self.policy_optimizer.step()\n",
    "\n",
    "                # check for nan\n",
    "                if torch.isnan(policy_loss):\n",
    "                    print(\"NaN detected in policy loss\")\n",
    "                    return\n",
    "\n",
    "                value_loss = torch.pow(self.value_net(\n",
    "                    batch_states) - batch_returns.unsqueeze(-1), 2).mean()\n",
    "\n",
    "                self.value_optimizer.zero_grad()\n",
    "                value_loss.backward()\n",
    "                self.value_optimizer.step()\n",
    "\n",
    "                # check for nan\n",
    "                if torch.isnan(value_loss):\n",
    "                    print(\"NaN detected in value loss\")\n",
    "                    return\n",
    "        \n",
    "        logger.add(policy_loss.item(), value_loss.item(), steps)\n",
    "        return (returns.mean(), returns.std(), steps)\n",
    "    \n",
    "    def record_best_effort(self):\n",
    "        env = gym.make('CartPole-v1', render_mode='rgb_array', max_episode_steps=10000)\n",
    "        env = gym.wrappers.RecordVideo(env, \"tests\")\n",
    "\n",
    "        state, _ = env.reset()\n",
    "        state = torch.tensor(state, dtype=TYPE, device=DEVICE, requires_grad=False)\n",
    "        env.start_video_recorder()\n",
    "\n",
    "        total_reward = 0\n",
    "        done, i = False, 0\n",
    "        \n",
    "        while not done and not truncated:\n",
    "            if self.quantized:\n",
    "                state = state.unsqueeze(0)\n",
    "            \n",
    "            action = self.policy_net.deterministic_action(state)\n",
    "            state, reward, done, truncated, _ = env.step(action)\n",
    "            state = torch.tensor(state, dtype=TYPE, device=DEVICE, requires_grad=False).unsqueeze(0)\n",
    "            total_reward += reward\n",
    "            i += 1\n",
    "\n",
    "        env.close()\n",
    "        return total_reward, i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = CartPoleQuantized(TYPE, DEVICE, record=False)\n",
    "session = PPOSession(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 50 - Returns: 26.469 - Std: 13.281 - Steps: 67.18\n",
      "Episode 100 - Returns: 32.258 - Std: 15.853 - Steps: 85.6\n",
      "Episode 150 - Returns: 37.799 - Std: 17.939 - Steps: 108.14\n",
      "Episode 200 - Returns: 48.887 - Std: 21.665 - Steps: 158.86\n",
      "Episode 250 - Returns: 54.155 - Std: 23.087 - Steps: 189.5\n",
      "Episode 300 - Returns: 59.424 - Std: 23.995 - Steps: 230.92\n",
      "Episode 350 - Returns: 68.847 - Std: 24.988 - Steps: 323.64\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-d4207a86b6c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-159-238a812120d1>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, episodes)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mreturns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mppo_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0mbatch_returns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-159-238a812120d1>\u001b[0m in \u001b[0;36mppo_step\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m                 \u001b[0;31m# Calculate new log probabilities\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m                 \u001b[0mnew_action_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_net\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m                 \u001b[0mnew_log_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_action_probs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_actions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-158-cee4c537c51d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mstochastic_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    202\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "session.run(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /home/ubuntu/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'truncated' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7dc58757ea2d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_best_effort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-14-49d488084d41>\u001b[0m in \u001b[0;36mrecord_best_effort\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquantized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m                 \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'truncated' referenced before assignment"
     ]
    }
   ],
   "source": [
    "session.record_best_effort()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and saving the new checkpoints!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save weights\n",
    "torch.save(session.policy_net.state_dict(), \"float-16-good-policy.pt\")\n",
    "torch.save(session.value_net.state_dict(), \"float-16-good-value.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/b0kch01/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/nn/modules/module.py:1518: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/training/torch.float32/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "element 0 of tensors does not require grad and does not have a grad_fn",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m quantize_dynamic(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     session\u001b[39m.\u001b[39mpolicy_net,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     {torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mLinear},\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mqint8,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m session\u001b[39m.\u001b[39mquantized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m session\u001b[39m.\u001b[39;49mrun(\u001b[39m500\u001b[39;49m)\n",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(episodes):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     returns, std, steps \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mppo_step()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode \u001b[39m%\u001b[39m \u001b[39m5\u001b[39m \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpisode \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mepisode\u001b[39m}\u001b[39;00m\u001b[39m - Returns: \u001b[39m\u001b[39m{\u001b[39;00mreturns\u001b[39m}\u001b[39;00m\u001b[39m - Std: \u001b[39m\u001b[39m{\u001b[39;00mstd\u001b[39m}\u001b[39;00m\u001b[39m - Steps: \u001b[39m\u001b[39m{\u001b[39;00msteps\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb Cell 15\u001b[0m line \u001b[0;36m9\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=87'>88</a>\u001b[0m policy_loss \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mtorch\u001b[39m.\u001b[39mmin(surrogate_loss, clipped_surrogate_loss)\u001b[39m.\u001b[39mmean()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=89'>90</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=90'>91</a>\u001b[0m policy_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=91'>92</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/ppo_base.ipynb#X20sZmlsZQ%3D%3D?line=93'>94</a>\u001b[0m value_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mpow(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue_net(batch_states) \u001b[39m-\u001b[39m batch_returns\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m), \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/pytorch-m1/lib/python3.10/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mRuntimeError\u001b[0m: element 0 of tensors does not require grad and does not have a grad_fn"
     ]
    }
   ],
   "source": [
    "# Load weights\n",
    "env = CartPoleQuantized(TYPE, DEVICE)\n",
    "session = PPOSession(env)\n",
    "session.policy_net.load_state_dict(torch.load(\"checkpoints/float-32-good-policy.pt\"))\n",
    "session.value_net.load_state_dict(torch.load(\"checkpoints/float-32-good-value.pt\"))\n",
    "\n",
    "session.policy_net.eval()\n",
    "session.value_net.eval()\n",
    "\n",
    "torch.backends.quantized.engine = \"qnnpack\"\n",
    "\n",
    "quantize_dynamic(\n",
    "    session.policy_net,\n",
    "    {torch.nn.Linear},\n",
    "    dtype=torch.qint8,\n",
    "    inplace=True\n",
    ")\n",
    "\n",
    "session.quantized = True\n",
    "session.run(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready /Users/b0kch01/Documents/Code/QuantizeRL/cart_pole/tests/rl-video-episode-0.mp4\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15321.0, 15321)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#session.record_best_effort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
